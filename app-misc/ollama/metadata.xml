<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE pkgmetadata SYSTEM "https://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
  <maintainer type="person">
    <!-- Ebuild from oiledmachine-overlay -->
    <email>orsonteodoro@hotmail.com</email>
    <name>Orson Teodoro</name>
  </maintainer>
  <maintainer type="person" proxied="yes">
    <!-- Ebuild originator -->
    <email>zdanevich.vitaly@ya.ru</email>
    <name>Vitaly Zdanevich</name>
  </maintainer>
  <!--

  oiledmachine-overlay:

  Ebuild obtained from guru.


  Ebuild build time environment variables

  OLLAMA_CONTEXT_LENGTH - Set the default context length.  For Open WebUI,
  you need at least 8192 for RAG support.
  Ebuild default: 2048

  OLLAMA_KV_CACHE_TYPE - The default quantization used for the VRAM.
  This value can be used to decrease memory use by half or more if q8_0 or
  q4_0 is used.  This value may be changed via /etc/conf.d/ollama.conf
  Valid values: fp16, q8_0, q4_0
  Upstream default: fp16

  OLLAMA_MAX_RETRIES - The number of times to retry downloading models.
  Default: 6

  OLLAMA_NUM_DOWNLOAD_PARTS - The number of download parts to split the model.
  It requires N * MB bandwidth.  If set too high, it will be too greedy and
  unfair for other network apps.  It will cause download restarts if too high.
  Upstream default: 16
  Ebuild default: 1

  OLLAMA_MIN_DOWNLOAD_PART_SIZE - The size of a small download part in MB.
  It must have no fractional part.
  Upstream default: 100
  Ebuild default: 1

  OLLAMA_MAX_DOWNLOAD_PART_SIZE - The size of a large download part in MB.
  If too large, it may a huge rewind to the start of that part.
  It must have no fractional part.
  Upstream default: 1000
  Ebuild default: 2

  Example

  Contents of /etc/portage/env/ollama.conf

  OLLAMA_CONTEXT_LENGTH=8192
  OLLAMA_MAX_RETRIES=7
  OLLAMA_NUM_DOWNLOAD_PARTS=2
  OLLAMA_MIN_DOWNLOAD_PART_SIZE=1
  OLLAMA_MAX_DOWNLOAD_PART_SIZE=5


  Contents of /etc/portage/package.env

  app-misc/ollama ollama.conf

  -->
  <upstream>
    <remote-id type="github">ollama/ollama</remote-id>
  </upstream>
  <use>
    <flag name="cuda">
      Support CUDA® for NVIDIA® GPUs.
    </flag>
    <flag name="blis">
      Support CPU optimized linear math via BLIS.
    </flag>
    <flag name="chroot">
      Isolate the ollama daemon filesystem instance to limit breach.
      (EXPERIMENTAL)
    </flag>
    <flag name="emoji">
      Support emojis.
    </flag>
    <flag name="flash">
      Use the FlashAttention optimization to reduce memory use and speed up
      inference instead of using standard attention.  Also, support less common
      quantizations.
    </flag>
    <flag name="lapack">
      Support CPU optimized linear math via the older LAPACK.
    </flag>
    <flag name="mkl">
      Support CPU optimized linear math via oneMKL for Intel® CPUs/GPUs.
    </flag>
    <flag name="native">
      Autodetect CPU instruction sets during build time for vendored llama.cpp
      used for inference for the old runner.
    </flag>
    <flag name="openblas">
      Support CPU optimized linear math via OpenBLAS.
    </flag>
    <flag name="rocm">
      Support ROCm™ for AMD GPUs.
    </flag>
    <flag name="sandbox">
      Isolate the ollama daemon filesystem instance to limit reconnasiance and
      information disclosure
      (EXPERIMENTAL)
    </flag>
    <flag name="unrestrict">
      Allow unrestricted downloading of ollama models.  Otherwise, only allow
      whitelisted models for downloading.
    </flag>
    <flag name="video_cards_intel">
      Support SYCL MKL acceleration for Intel® GPUs.
      (EXPERIMENTAL)
    </flag>
  </use>
</pkgmetadata>
