diff '--color=auto' -urp LocalAI-2.28.0.orig/Makefile LocalAI-2.28.0/Makefile
--- LocalAI-2.28.0.orig/Makefile	2025-04-15 13:00:51.000000000 -0700
+++ LocalAI-2.28.0/Makefile	2025-04-20 23:57:20.705015893 -0700
@@ -40,6 +40,7 @@ CUDA_LIBPATH?=/usr/local/cuda/lib64/
 GO_TAGS?=
 BUILD_ID?=
 NATIVE?=false
+OFFLINE?=true
 
 TEST_DIR=/tmp/test
 
@@ -214,16 +215,18 @@ ifeq ($(BUILD_API_ONLY),true)
 	GRPC_BACKENDS=
 endif
 
-.PHONY: all test build vendor get-sources prepare-sources prepare
+.PHONY: all test build vendor get-sources prepare-sources prepare sources/onnxruntime
 
 all: help
 
 ## bark.cpp
 sources/bark.cpp:
+ifeq ($(OFFLINE),false)
 	git clone --recursive $(BARKCPP_REPO) sources/bark.cpp && \
 	cd sources/bark.cpp && \
 	git checkout $(BARKCPP_VERSION) && \
 	git submodule update --init --recursive --depth 1 --single-branch
+endif
 
 sources/bark.cpp/build/libbark.a: sources/bark.cpp
 	cd sources/bark.cpp && \
@@ -250,10 +253,12 @@ sources/go-piper/libpiper_binding.a: sou
 
 ## stablediffusion (ggml)
 sources/stablediffusion-ggml.cpp:
+ifeq ($(OFFLINE),false)
 	git clone --recursive $(STABLEDIFFUSION_GGML_REPO) sources/stablediffusion-ggml.cpp && \
 	cd sources/stablediffusion-ggml.cpp && \
 	git checkout $(STABLEDIFFUSION_GGML_VERSION) && \
 	git submodule update --init --recursive --depth 1 --single-branch
+endif
 
 backend/go/image/stablediffusion-ggml/libsd.a: sources/stablediffusion-ggml.cpp
 	$(MAKE) -C backend/go/image/stablediffusion-ggml build/libstable-diffusion.a
@@ -264,7 +269,9 @@ backend-assets/grpc/stablediffusion-ggml
 
 sources/onnxruntime:
 	mkdir -p sources/onnxruntime
+ifeq ($(OFFLINE),false)
 	curl -L https://github.com/microsoft/onnxruntime/releases/download/v$(ONNX_VERSION)/onnxruntime-$(ONNX_OS)-$(ONNX_ARCH)-$(ONNX_VERSION).tgz -o sources/onnxruntime/onnxruntime-$(ONNX_OS)-$(ONNX_ARCH)-$(ONNX_VERSION).tgz
+endif
 	cd sources/onnxruntime && tar -xvf onnxruntime-$(ONNX_OS)-$(ONNX_ARCH)-$(ONNX_VERSION).tgz && rm onnxruntime-$(ONNX_OS)-$(ONNX_ARCH)-$(ONNX_VERSION).tgz
 	cd sources/onnxruntime && mv onnxruntime-$(ONNX_OS)-$(ONNX_ARCH)-$(ONNX_VERSION)/* ./
 
@@ -410,10 +417,12 @@ run: prepare ## run local-ai
 test-models/testmodel.ggml:
 	mkdir test-models
 	mkdir test-dir
+ifeq ($(OFFLINE),false)
 	wget -q https://huggingface.co/RichardErkhov/Qwen_-_Qwen2-1.5B-Instruct-gguf/resolve/main/Qwen2-1.5B-Instruct.Q2_K.gguf -O test-models/testmodel.ggml
 	wget -q https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin -O test-models/whisper-en
 	wget -q https://huggingface.co/mudler/all-MiniLM-L6-v2/resolve/main/ggml-model-q4_0.bin -O test-models/bert
 	wget -q https://cdn.openai.com/whisper/draft-20220913a/micro-machines.wav -O test-dir/audio.wav
+endif
 	cp tests/models_fixtures/* test-models
 
 prepare-test: grpcs
@@ -433,7 +442,9 @@ test: prepare test-models/testmodel.ggml
 prepare-e2e:
 	mkdir -p $(TEST_DIR)
 	cp -rfv $(abspath ./tests/e2e-fixtures)/gpu.yaml $(TEST_DIR)/gpu.yaml
+ifeq ($(OFFLINE),false)
 	test -e $(TEST_DIR)/ggllm-test-model.bin || wget -q https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q2_K.gguf -O $(TEST_DIR)/ggllm-test-model.bin
+endif
 	docker build --build-arg GRPC_BACKENDS="$(GRPC_BACKENDS)" --build-arg IMAGE_TYPE=core --build-arg BUILD_TYPE=$(BUILD_TYPE) --build-arg CUDA_MAJOR_VERSION=12 --build-arg CUDA_MINOR_VERSION=0 --build-arg FFMPEG=true -t localai-tests .
 
 run-e2e-image:
diff '--color=auto' -urp LocalAI-2.28.0.orig/backend/cpp/llama/Makefile LocalAI-2.28.0/backend/cpp/llama/Makefile
--- LocalAI-2.28.0.orig/backend/cpp/llama/Makefile	2025-04-15 13:00:51.000000000 -0700
+++ LocalAI-2.28.0/backend/cpp/llama/Makefile	2025-04-20 23:57:00.675655149 -0700
@@ -6,6 +6,7 @@ CMAKE_ARGS?=
 BUILD_TYPE?=
 ONEAPI_VARS?=/opt/intel/oneapi/setvars.sh
 TARGET?=--target grpc-server
+OFFLINE?=true
 
 # Disable Shared libs as we are linking on static gRPC and we can't mix shared and static
 CMAKE_ARGS+=-DBUILD_SHARED_LIBS=OFF -DLLAMA_CURL=OFF
@@ -51,6 +52,7 @@ ifeq ($(BUILD_TYPE),sycl_f32)
 endif
 
 llama.cpp:
+ifeq ($(OFFLINE),false)
 	mkdir -p llama.cpp
 	cd llama.cpp && \
 	git init && \
@@ -58,6 +60,7 @@ llama.cpp:
 	git fetch origin && \
 	git checkout -b build $(LLAMA_VERSION) && \
 	git submodule update --init --recursive --depth 1 --single-branch
+endif
 
 llama.cpp/examples/grpc-server: llama.cpp
 	mkdir -p llama.cpp/examples/grpc-server
