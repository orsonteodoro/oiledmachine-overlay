<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
  <maintainer type="person">
    <!-- Ebuild on the oiledmachine-overlay -->
    <email>orsonteodoro@hotmail.com</email>
    <name>Orson Teodoro</name>
  </maintainer>
  <!--

    Build time environment variables:

    LOCAL_AI_HOSTNAME - hostname
    Default:  127.0.0.1

    LOCAL_AI_PORT - port to connect to the web interface
    Default:  8080

    LOCAL_AI_URI - hostname to use
    Default:  http://127.0.0.1:8080

    Sample /etc/portage/env/local-ai.conf:

    LOCAL_AI_HOSTNAME="127.0.0.1"
    LOCAL_AI_PORT=8080
    LOCAL_AI_URI="http://127.0.0.1:8080"

    Sample /etc/portage/package.env:
    dev-python/local-ai local-ai.conf

    Inference support

    | Backend              | Text inference      | Image inference            | CPU | GPU | GPU SDK                            |
    | ---                  | ---                 | ---                        | --- | --- | ---                                |
    | diffusers            | N                   | Y (Image/video generation) | Y   | Y   | CUDA, ROCm, Intel, Metal           |
    | exllama2             | Y                   | N                          | N   | Y   | CUDA                               |
    | huggingface          | Y (Embeddings)      | N                          | Y   | Y   |                                    |
    | llama.cpp            | Y                   | N                          | Y   | Y   | CUDA, ROCm, SYCL, Vulkan, Metal    |
    | MLX                  | Y                   | N                          | N   | Y   | Metal - Apple Silicon              |
    | MLX-VLM              | Y (Vision-Language) | Y (Image understanding)    | N   | Y   | Metal - Apple Silicon              |
    | rfdetr               | N                   | Y (Object detection)       | Y   | Y   | CUDA                               |
    | stablediffusion-ggml | N                   | Y (Image generation)       | Y   | Y   | CUDA, SYCL, Vulkan                 |
    | transformers         | Y                   | N                          | Y   | Y   | CUDA, ROCm, Intel                  |
    | vLLM                 | Y                   | N                          | N   | Y   | CUDA, ROCm, Intel                  |

    

  -->
  <upstream>
    <bugs-to>https://github.com/mudler/LocalAI/issues</bugs-to>
    <remote-id type="github">mudler/LocalAI</remote-id>
  </upstream>
  <use>
    <flag name="ci">
      For upstream development only.  (DO NOT USE)
    </flag>
    <flag name="clblas">
      Support AMD/Intel GPU acceleration
    </flag>
    <flag name="devcontainer">
      For upstream development only.  (DO NOT USE)
    </flag>
    <flag name="cuda">
      Support NVIDIA GPU accelerated BLAS.
    </flag>
    <flag name="native">
      Build with -march=native.  Disabling requires SSE4.2 for ARCH=amd64.
    </flag>
    <flag name="openblas">
      Support CPU accelerated BLAS.
    </flag>
    <flag name="p2p">
      Support peer-to-peer inference.
    </flag>
    <flag name="rag">
      Support basic RAG (Retrieval-Augmented Generation).
    </flag>
    <flag name="stt">
      Support speech to text or microphone support.
    </flag>
    <flag name="tts">
      Support text to speech or audio out support.
    </flag>
    <flag name="sycl-f16">
      Support Intel GPU acceleration with 16 bit floats for speed.
    </flag>
    <flag name="sycl-f32">
      Support Intel GPU acceleration with 32 bit floats for accuracy.
    </flag>
    <flag name="rocm">
      Support AMD GPU accelerated BLAS.
    </flag>
  </use>
</pkgmetadata>
