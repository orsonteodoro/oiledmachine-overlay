<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE pkgmetadata SYSTEM "https://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
  <maintainer type="person">
    <!-- Ebuild originators -->
    <email>tupone@gentoo.org</email>
    <name>Tupone Alfredo</name>
  </maintainer>
  <maintainer type="person" proxied="yes">
    <!-- Ebuild originators -->
    <email>telans@posteo.de</email>
    <name>James Beddek</name>
  </maintainer>
  <!--

  oiledmachine-overlay notes:

  Ebuild availability based on GPU libs:

  Pytorch 2.9:  CUDA 12.6, CUDA 12.8, CUDA 13.0, ROCm 6.4
  Pytorch 2.8:  CUDA 12.6, CUDA 12.8, CUDA 12.9, ROCm 6.4
  Pytorch 2.7:  CUDA 11.8; Dropped based on inconsistency between upstream docs and cuDNN ebuild

  Footnotes:

  [1] QNNPACK, FBGEMM, oneDNN differences are listed in here.
  [2] x86 quantization is optimized/evaluated situationally based on either FBGEMM or oneDNN.
  [3] Reassurance that gloo is not optimized for GPU.

  Sources:

  [1] https://pytorch.org/docs/stable/quantization.html
  [2] https://github.com/pytorch/pytorch/issues/83888
  [3] https://pytorch.org/docs/stable/distributed.html

  -->
  <use>
    <flag name="cuda">
      Add support for CUDA® processing
    </flag>
    <flag name="distributed">
      Support distributed applications
    </flag>
    <flag name="eigen">
      Use Eigen as the fallback for BLAS.  Otherwise, it will fallback to the
      generic implementation.
    </flag>
    <flag name="fbgemm">
      Use FBGEMM as a x86 quantization backend.
    </flag>
    <flag name="ffmpeg">
      Add support for video processing operators
    </flag>
    <flag name="flash-attention">
      Add support for GPU optimized flash-attention for speed and reduced memory
      consumption.
    </flag>
    <flag name="gloo">
      Add multiple NUMA node or minimal CUDA® GPU capabilities but recommended
      for distributed CPU training but not GPU training.
    </flag>
    <flag name="kineto">
      Add CPU/GPU profiling support
    </flag>
    <flag name="magma">
      Add support for a GPU based linear algebra solver/factorizer.
    </flag>
    <flag name="mimalloc">
      Use the performance based memory allocator.
    </flag>
    <flag name="mkl">
      Use MKL as the BLAS provider
    </flag>
    <flag name="nnpack">
      Use NNPACK
    </flag>
    <flag name="mpi">
      Support CPU distributed training.
    </flag>
    <flag name="numpy">
      Add support for NumPy compatibility.
    </flag>
    <flag name="nccl">
      Support multiple NVIDIA® GPUs for distributed GPU training.
    </flag>
    <flag name="rccl">
      Support multiple AMD GPUs for distributed GPU training.
    </flag>
    <flag name="roctracer">
      Support tracing PyTorch annotated applications for ROCm™ builds.
    </flag>
    <flag name="onednn">
      Use FBGEMM as a x86 quantization backend  for the latest CPUs and/or
      Intel® GPUs.
    </flag>
    <flag name="openblas">
      Use OpenBLAS, for CPU optimized portability, as the BLAS provider.
    </flag>
    <flag name="opencl">
      Use OpenCL
    </flag>
    <flag name="opencv">
      Add support for image processing operators
    </flag>
    <flag name="openmp">
      Use OpenMP for parallel code
    </flag>
    <flag name="ssl">
      Use ssl for node to node communication.
    </flag>
    <flag name="system-libs">
      Use system libs; otherwise, use vendored/bundled packages.
      This USE flag is the same as the vanilla USE flag.

      For correctness and performance, disable this USE flag.
      For security improvement, enable this USE flag.
    </flag>
    <flag name="qnnpack">
      Use QNNPACK as an optimized ARM64 quantization backend.
    </flag>
    <flag name="xnnpack">
      Use XNNPACK for optimized neural network inference operations for the CPU.
    </flag>
  </use>
  <upstream>
    <remote-id type="github">pytorch/pytorch</remote-id>
  </upstream>
</pkgmetadata>
