diff '--color=auto' -urp pytorch-2.1.2.orig/benchmarks/dynamo/_onnx/bench.sh pytorch-2.1.2/benchmarks/dynamo/_onnx/bench.sh
--- pytorch-2.1.2.orig/benchmarks/dynamo/_onnx/bench.sh	2023-12-12 08:41:07.000000000 -0800
+++ pytorch-2.1.2/benchmarks/dynamo/_onnx/bench.sh	2024-08-11 00:43:52.515647944 -0700
@@ -71,7 +71,7 @@ echo "Running benchmarking onnx w/ torch
 echo "Benchmark logs will be saved under pytorch/$log_folder"
 
 # NOTE: --quick is handy to run on small subset of ~3 models for quick sanity check.
-(set -x; PATH=/usr/local/cuda/bin/:$PATH python benchmarks/dynamo/runner.py \
+(set -x; PATH=/opt/cuda/bin/:$PATH python benchmarks/dynamo/runner.py \
     --suites=torchbench \
     --suites=huggingface \
     --suites=timm_models \
diff '--color=auto' -urp pytorch-2.1.2.orig/cmake/Modules/FindCUDAToolkit.cmake pytorch-2.1.2/cmake/Modules/FindCUDAToolkit.cmake
--- pytorch-2.1.2.orig/cmake/Modules/FindCUDAToolkit.cmake	2023-12-12 08:41:07.000000000 -0800
+++ pytorch-2.1.2/cmake/Modules/FindCUDAToolkit.cmake	2024-08-11 00:41:42.997776616 -0700
@@ -656,7 +656,7 @@ else()
 
     # Force the global default /usr/local/cuda to the front on Unix.
     if(UNIX)
-      list(INSERT search_paths 0 "/usr/local/cuda")
+      list(INSERT search_paths 0 "/opt/cuda")
     endif()
 
     # Now search for the toolkit again using the platform default search paths.
diff '--color=auto' -urp pytorch-2.1.2.orig/scripts/fbcode-dev-setup/ccache_setup.sh pytorch-2.1.2/scripts/fbcode-dev-setup/ccache_setup.sh
--- pytorch-2.1.2.orig/scripts/fbcode-dev-setup/ccache_setup.sh	2023-12-12 08:41:07.000000000 -0800
+++ pytorch-2.1.2/scripts/fbcode-dev-setup/ccache_setup.sh	2024-08-11 00:43:41.095835639 -0700
@@ -40,8 +40,8 @@ done
 set +e
 nvcc_path=$(which nvcc)
 if [[ -z "$nvcc_path" ]]; then
-  nvcc_path="/usr/local/cuda/bin/nvcc"
-  export PATH="/usr/local/cuda/bin:$PATH"
+  nvcc_path="/opt/cuda/bin/nvcc"
+  export PATH="/opt/cuda/bin:$PATH"
 fi
 set -e
 if [ ! -f "$nvcc_path" ] && ! $force; then
diff '--color=auto' -urp pytorch-2.1.2.orig/torch/utils/collect_env.py pytorch-2.1.2/torch/utils/collect_env.py
--- pytorch-2.1.2.orig/torch/utils/collect_env.py	2023-12-12 08:41:07.000000000 -0800
+++ pytorch-2.1.2/torch/utils/collect_env.py	2024-08-11 00:42:06.181395597 -0700
@@ -163,7 +163,7 @@ def get_cudnn_version(run_lambda):
         # https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html#install
         # https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installmac
         # Use CUDNN_LIBRARY when cudnn library is installed elsewhere.
-        cudnn_cmd = 'ls /usr/local/cuda/lib/libcudnn*'
+        cudnn_cmd = 'ls /opt/cuda/lib64/libcudnn*'
     else:
         cudnn_cmd = 'ldconfig -p | grep libcudnn | rev | cut -d" " -f1 | rev'
     rc, out, _ = run_lambda(cudnn_cmd)
diff '--color=auto' -urp pytorch-2.1.2.orig/torch/utils/cpp_extension.py pytorch-2.1.2/torch/utils/cpp_extension.py
--- pytorch-2.1.2.orig/torch/utils/cpp_extension.py	2023-12-12 08:41:07.000000000 -0800
+++ pytorch-2.1.2/torch/utils/cpp_extension.py	2024-08-11 00:41:48.857680310 -0700
@@ -112,7 +112,7 @@ def _find_cuda_home() -> Optional[str]:
                 else:
                     cuda_home = cuda_homes[0]
             else:
-                cuda_home = '/usr/local/cuda'
+                cuda_home = '/opt/cuda'
             if not os.path.exists(cuda_home):
                 cuda_home = None
     if cuda_home and not torch.cuda.is_available():
