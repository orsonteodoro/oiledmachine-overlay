Patch Status: Requires testing on aesni hardware.  Report back if it works by
              sending a git issue to oiledmachine-overlay.
Patch Author: Orson Teodoro <orsonteodoro@hotmail.com>
Date: Jan 11, 2021
v2.2:  Fixed ECB, CBC, CTR, XTS glue with TRESOR AES-128
       Merged tresor-glue-remove-xts-casts-and-api-updates-for-5.6-aesni.patch
v2.1:  Moved running ctr(tresor), xts(tresor) tests to arch/x86/crypto/tresor_key.c.
v2:  Limit XTS to 128-bit cipher key, and 128-bit tweak key.
     Change location of the XTS tweak key from db4-db7 to db2-db3.
     Remove unused rounds for XTS tweak key.
     Updated testmgr.c for ctr(tresor), xts(tresor).
     Update init/main.c to run ctr(tresor), xts(tresor) tests.
     Fixed .cra_name for xts(tresor) that prevented opening dm-crypt devices.
     Added suffixes to assembly instructions.
v1.1:  Use key_length for encryption and decryption for XTS.
v1:  Initial release.
TODO
Check if tresor_setkey_xts_tweak is required for crypto_skcipher_setkey
  consumers (See crypto/testmgr.c for details) for applications / 
  crypto-drivers / crypto-libraries
----
diff -urp linux-5.10.6-ot.orig/arch/x86/crypto/tresor_asm.S linux-5.10.6-ot/arch/x86/crypto/tresor_asm.S
--- linux-5.10.6-ot.orig/arch/x86/crypto/tresor_asm.S	2021-01-11 22:38:07.573013902 -0800
+++ linux-5.10.6-ot/arch/x86/crypto/tresor_asm.S	2021-01-11 22:43:19.268879070 -0800
@@ -25,9 +25,8 @@
 /* 64-bit debug registers */
 .set	db0,	%db0	/* round key 0a */
 .set	db1,	%db1	/* round key 0b */
-.set	db2,	%db2	/* round key 1a */
-.set	db3,	%db3	/* round key 1b */
-
+.set	db2,	%db2	/* round key 1a ^ xts tweak 0a */
+.set	db3,	%db3	/* round key 1b ^ xts tweak 0b */
 
 /* 128-bit SSE registers */
 .set	rstate,	%xmm0		/* AES state */
@@ -221,6 +220,17 @@
 .endm
 
 
+/* copy xts tweak key from dbg regs into xmm regs */
+.macro	read_key_xts_tweak r0 r1 rounds
+	movq	db2,%rax
+	movq	%rax,\r0
+	movq	db3,%rax
+	movq	%rax,rhelp
+	shufps	$0x44,rhelp,\r0
+	xorq	%rax,%rax	/* clear rax, as it contained key material */
+.endm
+
+
 /* Encrypt */
 .macro	encrypt_block rounds
 	movdqu	0(%rsi),rstate
@@ -249,6 +259,26 @@
 .endm
 
 
+/* Encrypt */
+.macro	encrypt_block_xts_tweak rounds
+	movdqu	0(%rsi),rstate
+	read_key_xts_tweak	rk0 rk1 \rounds
+	pxor		rk0,rstate
+	generate_rks_\rounds
+	aesenc		rk1,rstate
+	aesenc		rk2,rstate
+	aesenc		rk3,rstate
+	aesenc		rk4,rstate
+	aesenc		rk5,rstate
+	aesenc		rk6,rstate
+	aesenc		rk7,rstate
+	aesenc		rk8,rstate
+	aesenc		rk9,rstate
+	aesenclast	rk\rounds,rstate
+	epilog
+.endm
+
+
 /* Decrypt */
 .macro	decrypt_block rounds
 	movdqu	0(%rsi),rstate
@@ -295,6 +325,8 @@
 	.globl	tresor_decblk_192
 	.globl	tresor_encblk_256
 	.globl	tresor_decblk_256
+	.globl	tresor_set_key_xts_tweak
+	.globl	tresor_encblk_128_xts_tweak
 
 
 /* void tresor_encblk(u8 *out, const u8 *in) */
@@ -306,6 +338,11 @@ tresor_encblk_256:
 	encrypt_block	14
 
 
+/* void tresor_encblk_128_xts_tweak(u8 *out, const u8 *in) */
+tresor_encblk_128_xts_tweak:
+	encrypt_block_xts_tweak	10
+
+
 /* void tresor_decblk(u8 *out, const u8 *in) */
 tresor_decblk_128:
 	decrypt_block	10
@@ -317,6 +354,7 @@ tresor_decblk_256:
 
 /* void tresor_set_key(const u8 *in_key) */
 tresor_set_key:
+	pushq	%rax
 	movq	0(%rdi),%rax
 	movq	%rax,db0
 	movq	8(%rdi),%rax
@@ -325,7 +363,17 @@ tresor_set_key:
 	movq	%rax,db2
 	movq	24(%rdi),%rax
 	movq	%rax,db3
-	xorq	%rax,%rax	/* clear rax, as it contained key material */
+	popq	%rax
+	retq
+
+/* void tresor_set_key_xts_tweak(const u8 *in_key) */
+tresor_set_key_xts_tweak:
+	pushq	%rax
+	movq	0(%rdi),%rax
+	movq	%rax,db2
+	movq	8(%rdi),%rax
+	movq	%rax,db3
+	popq	%rax
 	retq
 
 #ifdef CONFIG_CRYPTO_TRESOR_KEY_VIA_CPU0
diff -urp linux-5.10.6-ot.orig/arch/x86/crypto/tresor_glue.c linux-5.10.6-ot/arch/x86/crypto/tresor_glue.c
--- linux-5.10.6-ot.orig/arch/x86/crypto/tresor_glue.c	2021-01-11 22:38:07.577014055 -0800
+++ linux-5.10.6-ot/arch/x86/crypto/tresor_glue.c	2021-01-11 22:59:39.191440931 -0800
@@ -5,6 +5,26 @@
  * Copyright (C) 2010	Tilo Mueller <tilo.mueller@informatik.uni-erlangen.de>
  * Copyright (C) 2012	Hans Spath <tresor@hans-spath.de>
  *
+ * Portions use skcipher cbc_, ecb_, ctr_ code from crypto/cast5_avx_glue.c by:
+ *   Copyright (C) 2012 Johannes Goetzfried
+ *       <Johannes.Goetzfried@informatik.stud.uni-erlangen.de>
+ *   Copyright © 2013 Jussi Kivilinna <jussi.kivilinna@iki.fi>
+ *
+ * Portions use aesni_init and aesni_skciphers code from crypto/aesni-intel_glue.c by:
+ *   Copyright (C) 2008, Intel Corp.
+ *      Author: Huang Ying <ying.huang@intel.com>
+ *
+ * Portions use xts glue code from crypto/cast6_avx_glue.c
+ *   Copyright (C) 2012 Johannes Goetzfried
+ *       <Johannes.Goetzfried@informatik.stud.uni-erlangen.de>
+ *   Copyright © 2013 Jussi Kivilinna <jussi.kivilinna@iki.fi>
+ *
+ * CBC & ECB parts based on code (crypto/cbc.c,ecb.c) by:
+ *   Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>
+ *
+ * CTR part based on code (crypto/ctr.c) by:
+ *   (C) Copyright IBM Corp. 2007 - Joy Latten <latten@us.ibm.com>
+ *
  * This program is free software; you can redistribute it and/or modify it
  * under the terms and conditions of the GNU General Public License,
  * version 2, as published by the Free Software Foundation.
@@ -17,10 +37,17 @@
  * You should have received a copy of the GNU General Public License along with
  * this program; if not, write to the Free Software Foundation, Inc., 59 Temple
  * Place - Suite 330, Boston, MA 02111-1307 USA.
+ *
+ * Portions include skcipher updates by Eric Biggers <ebiggers@google.com> and
+ * Herbert Xu <herbert@gondor.apana.org.au>.
  */
 
+#include <asm/crypto/glue_helper.h>
 #include <crypto/algapi.h>
+#include <crypto/b128ops.h>
+#include <crypto/internal/simd.h>
 #include <crypto/tresor.h>
+#include <crypto/xts.h>
 #include <linux/module.h>
 #include <crypto/aes.h>
 #include <linux/smp.h>
@@ -31,12 +58,16 @@
  */
 asmlinkage bool tresor_capable(void);
 asmlinkage void tresor_set_key(const u8 *in_key);
+asmlinkage void tresor_set_key_xts_tweak(const u8 *in_key);
 asmlinkage void tresor_encblk_128(u8 *out, const u8 *in);
 asmlinkage void tresor_decblk_128(u8 *out, const u8 *in);
 asmlinkage void tresor_encblk_192(u8 *out, const u8 *in);
 asmlinkage void tresor_decblk_192(u8 *out, const u8 *in);
 asmlinkage void tresor_encblk_256(u8 *out, const u8 *in);
 asmlinkage void tresor_decblk_256(u8 *out, const u8 *in);
+asmlinkage void tresor_encblk_128_xts_tweak(u8 *out, const u8 *in);
+asmlinkage void tresor_encblk_192_xts_tweak(u8 *out, const u8 *in);
+asmlinkage void tresor_encblk_256_xts_tweak(u8 *out, const u8 *in);
 
 
 
@@ -150,6 +181,346 @@ void tresor_setkey(const u8 *in_key)
 	on_each_cpu(tresor_setkey_current_cpu, (void *)in_key, 1);
 }
 
+/*
+ * Set XTS tweak key
+ */
+static void tresor_setkey_xts_tweak_current_cpu(void *data)
+{
+	printk(KERN_DEBUG "TRESOR: %s running on cpu %d\n",
+		__func__, smp_processor_id());
+	tresor_set_key_xts_tweak((const u8 *)data);
+}
+
+void tresor_setkey_xts_tweak(const u8 *in_key)
+{
+	on_each_cpu(tresor_setkey_xts_tweak_current_cpu, (void *)in_key, 1);
+}
+
+static int tresor_skcipher_setkey(struct crypto_skcipher *tfm, const u8 *key, unsigned int keylen)
+{
+	tresor_setdummykey(crypto_skcipher_tfm(tfm), key, keylen);
+	return 0;
+}
+
+static int ecb_crypt(struct skcipher_request *req, bool enc)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);
+	struct skcipher_walk walk;
+	const unsigned int bsize = AES_BLOCK_SIZE;
+	unsigned int nbytes;
+	void (*f)(u8 *dst, const u8 *src);
+	int err;
+	unsigned long irq_flags;
+
+	err = skcipher_walk_virt(&walk, req, true);
+
+	switch (ctx->key_length) {
+	case AES_KEYSIZE_128:
+		f = (enc) ? tresor_encblk_128 : tresor_decblk_128;
+		break;
+	case AES_KEYSIZE_192:
+		f = (enc) ? tresor_encblk_192 : tresor_decblk_192;
+		break;
+	case AES_KEYSIZE_256:
+		f = (enc) ? tresor_encblk_256 : tresor_decblk_256;
+		break;
+	}
+
+	while ((nbytes = walk.nbytes)) {
+		u8 *wsrc = walk.src.virt.addr;
+		u8 *wdst = walk.dst.virt.addr;
+
+		/* Handle leftovers */
+		do {
+			tresor_prolog(&irq_flags);
+			f(wdst, wsrc);
+			tresor_epilog(&irq_flags);
+			wsrc += bsize;
+			wdst += bsize;
+			nbytes -= bsize;
+		} while (nbytes >= bsize);
+
+		err = skcipher_walk_done(&walk, nbytes);
+	}
+
+	return err;
+}
+
+static int ecb_encrypt(struct skcipher_request *req)
+{
+	return ecb_crypt(req, true);
+}
+
+static int ecb_decrypt(struct skcipher_request *req)
+{
+	return ecb_crypt(req, false);
+}
+
+
+static int cbc_encrypt(struct skcipher_request *req)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);
+	const unsigned int bsize = AES_BLOCK_SIZE;
+	struct skcipher_walk walk;
+	unsigned int nbytes;
+	int err;
+	void (*f)(u8 *dst, const u8 *src);
+	unsigned long irq_flags;
+
+	err = skcipher_walk_virt(&walk, req, false);
+
+	switch (ctx->key_length) {
+	case AES_KEYSIZE_128:
+		f = tresor_encblk_128;
+		break;
+	case AES_KEYSIZE_192:
+		f = tresor_encblk_192;
+		break;
+	case AES_KEYSIZE_256:
+		f = tresor_encblk_256;
+		break;
+	}
+
+	while ((nbytes = walk.nbytes)) {
+		u128 *src = (u128 *)walk.src.virt.addr;
+		u128 *dst = (u128 *)walk.dst.virt.addr;
+		u128 *iv = (u128 *)walk.iv;
+
+		do {
+			u128_xor(dst, src, iv);
+			tresor_prolog(&irq_flags);
+			f((u8 *)dst, (u8 *)dst);
+			tresor_epilog(&irq_flags);
+			iv = dst;
+			src++;
+			dst++;
+			nbytes -= bsize;
+		} while (nbytes >= bsize);
+
+		*(u128 *)walk.iv = *iv;
+		err = skcipher_walk_done(&walk, nbytes);
+	}
+
+	return err;
+}
+
+static unsigned int __cbc_decrypt(struct crypto_aes_ctx *ctx,
+				  struct skcipher_walk *walk)
+{
+	const unsigned int bsize = AES_BLOCK_SIZE;
+	unsigned int nbytes = walk->nbytes;
+	u128*src = (u128 *)walk->src.virt.addr;
+	u128 *dst = (u128 *)walk->dst.virt.addr;
+	u128 last_iv;
+	void (*f)(u8 *dst, const u8 *src);
+	unsigned long irq_flags;
+
+	/* Start of the last block. */
+	src += nbytes / bsize - 1;
+	dst += nbytes / bsize - 1;
+
+	last_iv = *src;
+
+	switch (ctx->key_length) {
+	case AES_KEYSIZE_128:
+		f = tresor_decblk_128;
+		break;
+	case AES_KEYSIZE_192:
+		f = tresor_decblk_192;
+		break;
+	case AES_KEYSIZE_256:
+		f = tresor_decblk_256;
+		break;
+	}
+
+	/* Handle leftovers */
+	for (;;) {
+		tresor_prolog(&irq_flags);
+		f((u8 *)dst, (u8 *)src);
+		tresor_epilog(&irq_flags);
+
+		nbytes -= bsize;
+		if (nbytes < bsize)
+			break;
+
+		u128_xor(dst, dst, (src-1));
+		src -= 1;
+		dst -= 1;
+	}
+
+	u128_xor(dst, dst, walk->iv);
+	*(u128 *)walk->iv = last_iv;
+
+	return nbytes;
+}
+
+static int cbc_decrypt(struct skcipher_request *req)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);
+	struct skcipher_walk walk;
+	unsigned int nbytes;
+	int err;
+
+	err = skcipher_walk_virt(&walk, req, false);
+
+	while ((nbytes = walk.nbytes)) {
+		nbytes = __cbc_decrypt(ctx, &walk);
+		err = skcipher_walk_done(&walk, nbytes);
+	}
+	return err;
+}
+
+static void tresor_crypt_ctr(const void *ctx, u8 *d, const u8 *s, le128 *iv)
+{
+	be128 ctrblk;
+	u128 *dst = (u128 *)d;
+	const u128 *src = (const u128 *)s;
+	unsigned long irq_flags;
+
+	le128_to_be128(&ctrblk, iv);
+	le128_inc(iv);
+
+	tresor_prolog(&irq_flags);
+	switch (ctx->key_length) {
+	case AES_KEYSIZE_128:
+		tresor_encblk_128((u8 *)&ctrblk, (u8 *)&ctrblk);
+		break;
+	case AES_KEYSIZE_192:
+		tresor_encblk_192((u8 *)&ctrblk, (u8 *)&ctrblk);
+		break;
+	case AES_KEYSIZE_256:
+		tresor_encblk_256((u8 *)&ctrblk, (u8 *)&ctrblk);
+		break;
+	}
+	tresor_epilog(&irq_flags);
+
+	u128_xor(dst, src, (u128 *)&ctrblk);
+}
+
+static const struct common_glue_ctx tresor_ctr = {
+	.num_funcs = 1,
+	.fpu_blocks_limit = 1,
+
+	.funcs = { {
+		.num_blocks = 1,
+		.fn_u = { .ctr = tresor_crypt_ctr }
+	} }
+};
+
+static int ctr_crypt(struct skcipher_request *req)
+{
+	return glue_ctr_req_128bit(&tresor_ctr, req);
+}
+
+void __tresor_encrypt(const void *ctx, u8 *dst, const u8 *src) {
+	unsigned long irq_flags;
+
+	/* encrypt using the cipher key */
+	tresor_prolog(&irq_flags);
+	tresor_encblk_128(dst, src);
+	tresor_epilog(&irq_flags);
+}
+
+void __tresor_decrypt(const void *ctx, u8 *dst, const u8 *src) {
+	unsigned long irq_flags;
+
+	/* decrypt using the cipher key */
+	tresor_prolog(&irq_flags);
+	tresor_decblk_128(dst, src);
+	tresor_epilog(&irq_flags);
+}
+
+void __xts_tweak_tresor_encrypt(const void *ctx, u8 *dst, const u8 *src) {
+	unsigned long irq_flags;
+
+	/* encrypt using the tweak key */
+	tresor_prolog(&irq_flags);
+	tresor_encblk_128_xts_tweak(dst, src);
+	tresor_epilog(&irq_flags);
+}
+
+struct tresor_xts_ctx {
+	struct tresor_ctx crypt_ctx;
+	struct tresor_ctx tweak_ctx;
+};
+
+static int xts_tresor_setkey(struct crypto_skcipher *tfm, const u8 *key,
+			    unsigned int keylen)
+{
+	struct tresor_xts_ctx *ctx = crypto_skcipher_ctx(tfm);
+	int err;
+
+	keylen /= 2;
+
+	/* Limited hardware breakpoint address registers limits us to 128-bit */
+	if (keylen != AES_KEYSIZE_128)
+		return -EINVAL;
+
+	err = xts_verify_key(tfm, key, keylen*2);
+	if (err)
+		return err;
+
+	ctx->crypt_ctx.key_length = keylen;
+	ctx->tweak_ctx.key_length = keylen;
+
+	/* Same reason explained in tresor_setdummykey comment */
+	return 0;
+}
+
+static void tresor_xts_enc(const void *ctx, u8 *dst, const u8 *src, le128 *iv)
+{
+	glue_xts_crypt_128bit_one(ctx, dst, src, iv,
+				  __tresor_encrypt);
+}
+
+static void tresor_xts_dec(const void *ctx, u8 *dst, const u8 *src, le128 *iv)
+{
+	glue_xts_crypt_128bit_one(ctx, dst, src, iv,
+				  __tresor_decrypt);
+}
+
+static const struct common_glue_ctx tresor_enc_xts = {
+	.num_funcs = 1,
+	.fpu_blocks_limit = 1,
+
+	.funcs = { {
+		.num_blocks = 1,
+		.fn_u = { .xts = tresor_xts_enc }
+	} }
+};
+
+static const struct common_glue_ctx tresor_dec_xts = {
+	.num_funcs = 1,
+	.fpu_blocks_limit = 1,
+
+	.funcs = { {
+		.num_blocks = 1,
+		.fn_u = { .xts = tresor_xts_dec }
+	} }
+};
+
+static int xts_encrypt(struct skcipher_request *req)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct tresor_xts_ctx *ctx = crypto_skcipher_ctx(tfm);
+
+	return glue_xts_req_128bit(&tresor_enc_xts, req,
+				   __xts_tweak_tresor_encrypt,
+				   &ctx->tweak_ctx, &ctx->crypt_ctx, false);
+}
+
+static int xts_decrypt(struct skcipher_request *req)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct tresor_xts_ctx *ctx = crypto_skcipher_ctx(tfm);
+
+	return glue_xts_req_128bit(&tresor_dec_xts, req,
+				   __xts_tweak_tresor_encrypt,
+				   &ctx->tweak_ctx, &ctx->crypt_ctx, true);
+}
 
 /*
  * Crypto API algorithm
@@ -175,12 +546,96 @@ static struct crypto_alg tresor_alg = {
 	}
 };
 
+static struct skcipher_alg tresor_skciphers[] = {
+	{
+		.base = {
+			.cra_name		= "__ecb(tresor)",
+			.cra_driver_name	= "__ecb-tresor-aesni",
+			.cra_priority		= 400,
+			.cra_flags		= CRYPTO_ALG_INTERNAL,
+			.cra_blocksize		= AES_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct crypto_aes_ctx),
+			.cra_module		= THIS_MODULE,
+		},
+		.min_keysize	= AES_MIN_KEY_SIZE,
+		.max_keysize	= AES_MAX_KEY_SIZE,
+		.setkey		= tresor_skcipher_setkey,
+		.encrypt	= ecb_encrypt,
+		.decrypt	= ecb_decrypt,
+	}, {
+		.base = {
+			.cra_name		= "__cbc(tresor)",
+			.cra_driver_name	= "__cbc-tresor-aesni",
+			.cra_priority		= 400,
+			.cra_flags		= CRYPTO_ALG_INTERNAL,
+			.cra_blocksize		= AES_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct crypto_aes_ctx),
+			.cra_module		= THIS_MODULE,
+		},
+		.min_keysize	= AES_MIN_KEY_SIZE,
+		.max_keysize	= AES_MAX_KEY_SIZE,
+		.ivsize		= AES_BLOCK_SIZE,
+		.setkey		= tresor_skcipher_setkey,
+		.encrypt	= cbc_encrypt,
+		.decrypt	= cbc_decrypt,
+#ifdef CONFIG_X86_64
+	}, {
+		.base = {
+			.cra_name		= "__ctr(tresor)",
+			.cra_driver_name	= "__ctr-tresor-aesni",
+			.cra_priority		= 400,
+			.cra_flags		= CRYPTO_ALG_INTERNAL,
+			.cra_blocksize		= 1,
+			.cra_ctxsize		= sizeof(struct crypto_aes_ctx),
+			.cra_module		= THIS_MODULE,
+		},
+		.min_keysize	= AES_MIN_KEY_SIZE,
+		.max_keysize	= AES_MAX_KEY_SIZE,
+		.ivsize		= AES_BLOCK_SIZE,
+		.chunksize	= AES_BLOCK_SIZE,
+		.setkey		= tresor_skcipher_setkey,
+		.encrypt	= ctr_crypt,
+		.decrypt	= ctr_crypt,
+	}, {
+		.base = {
+			.cra_name		= "__xts(tresor)",
+			.cra_driver_name	= "__xts-tresor-aesni",
+			.cra_priority		= 401,
+			.cra_flags		= CRYPTO_ALG_INTERNAL,
+			.cra_blocksize		= AES_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct tresor_xts_ctx),
+			.cra_module		= THIS_MODULE,
+		},
+		.min_keysize	= 2 * AES_MIN_KEY_SIZE,
+		.max_keysize	= 2 * AES_MIN_KEY_SIZE,
+		.ivsize		= AES_BLOCK_SIZE,
+		.setkey		= xts_tresor_setkey,
+		.encrypt	= xts_encrypt,
+		.decrypt	= xts_decrypt,
+#endif
+	}
+};
+
+static struct simd_skcipher_alg *tresor_simd_skciphers[ARRAY_SIZE(tresor_skciphers)];
 
 /* Initialize module */
 static int __init tresor_init(void)
 {
 	int retval;
 	retval = crypto_register_alg(&tresor_alg);
+	if (retval)
+		return retval;
+
+	retval = simd_register_skciphers_compat(tresor_skciphers,
+					     ARRAY_SIZE(tresor_skciphers),
+					     tresor_simd_skciphers);
+	if (retval)
+		goto unregister_cipher;
+
+	return 0;
+
+unregister_cipher:
+	crypto_unregister_alg(&tresor_alg);
 	return retval;
 }
 module_init(tresor_init);
@@ -189,6 +644,8 @@ module_init(tresor_init);
 /* Remove module */
 static void __exit tresor_fini(void)
 {
+	simd_unregister_skciphers(tresor_skciphers, ARRAY_SIZE(tresor_skciphers),
+				  tresor_simd_skciphers);
 	crypto_unregister_alg(&tresor_alg);
 }
 module_exit(tresor_fini);
diff -urp linux-5.10.6-ot.orig/arch/x86/crypto/tresor_key.c linux-5.10.6-ot/arch/x86/crypto/tresor_key.c
--- linux-5.10.6-ot.orig/arch/x86/crypto/tresor_key.c	2021-01-11 22:38:08.101034057 -0800
+++ linux-5.10.6-ot/arch/x86/crypto/tresor_key.c	2021-01-11 22:40:02.573398507 -0800
@@ -627,6 +627,8 @@ void tresor_kernel_init(void)
 	tresor_unlock_tests();
 	alg_test("ecb(tresor)", "ecb(tresor)", 0, 0);
 	alg_test("cbc(tresor)", "cbc(tresor)", 0, 0);
+	alg_test("ctr(tresor)", "ctr(tresor)", 0, 0);
+	alg_test("xts(tresor)", "xts(tresor)", 0, 0);
 	tresor_lock_tests();
 #endif
 
diff -urp linux-5.10.6-ot.orig/crypto/testmgr.c linux-5.10.6-ot/crypto/testmgr.c
--- linux-5.10.6-ot.orig/crypto/testmgr.c	2021-01-11 22:38:08.089033599 -0800
+++ linux-5.10.6-ot/crypto/testmgr.c	2021-01-11 22:40:02.573398507 -0800
@@ -2761,8 +2761,11 @@ static int test_skcipher_vec_cfg(const c
 		crypto_skcipher_clear_flags(tfm,
 					    CRYPTO_TFM_REQ_FORBID_WEAK_KEYS);
 #ifdef CONFIG_CRYPTO_TRESOR
-	if (strstr(driver, "tresor"))
+	if (strstr(driver, "tresor")) {
 		tresor_setkey(vec->key);
+		if (strstr(driver, "xts"))
+			tresor_setkey_xts_tweak(vec->key + vec->klen/2);
+	}
 #endif
 	err = do_setkey(crypto_skcipher_setkey, tfm, vec->key, vec->klen,
 			cfg, alignmask);
@@ -4655,6 +4658,15 @@ static const struct alg_test_desc alg_te
 			.cipher = __VECS(sm4_ctr_tv_template)
 		}
 	}, {
+#ifdef CONFIG_CRYPTO_TRESOR
+		.alg = "ctr(tresor)",
+		.test = alg_test_skcipher,
+		.fips_allowed = 1,
+		.suite = {
+			.cipher = __VECS(aes_ctr_tv_template)
+		}
+	}, {
+#endif
 		.alg = "ctr(twofish)",
 		.test = alg_test_skcipher,
 		.suite = {
@@ -5565,6 +5577,16 @@ static const struct alg_test_desc alg_te
 			.cipher = __VECS(serpent_xts_tv_template)
 		}
 	}, {
+#ifdef CONFIG_CRYPTO_TRESOR
+		.alg = "xts(tresor)",
+		.generic_driver = "xts(ecb(aes-generic))",
+		.test = alg_test_skcipher,
+		.fips_allowed = 1,
+		.suite = {
+			.cipher = __VECS(aes_xts_tv_template)
+		}
+	}, {
+#endif
 		.alg = "xts(twofish)",
 		.generic_driver = "xts(ecb(twofish-generic))",
 		.test = alg_test_skcipher,
Only in linux-5.10.6-ot/crypto: testmgr.c.orig
diff -urp linux-5.10.6-ot.orig/include/crypto/tresor.h linux-5.10.6-ot/include/crypto/tresor.h
--- linux-5.10.6-ot.orig/include/crypto/tresor.h	2021-01-11 22:38:07.581014208 -0800
+++ linux-5.10.6-ot/include/crypto/tresor.h	2021-01-11 22:40:02.573398507 -0800
@@ -10,10 +10,15 @@
 /* number of chars to clear memory */
 #define TRESOR_RANDOM_CHARS 4096
 
+struct tresor_ctx {
+	int key_length;
+};
+
 /* TRESOR core functionality (enc, dec, setkey) */
 void tresor_encrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src);
 void tresor_decrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src);
 void tresor_setkey(const u8 *in_key);
+void tresor_setkey_xts_tweak(const u8 *in_key);
 bool tresor_capable(void);
 
 void tresor_kernel_init(void);
