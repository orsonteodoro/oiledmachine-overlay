Patch Status: Work In Progress / Incomplete
Patch Author: Orson Teodoro <orsonteodoro@hotmail.com>
Date: Jul 2, 2020
TODOs:
setkey tresor-xts for dmcrypt
handle keysize with xts
----
diff -urp linux-5.4.49-ot.orig/arch/x86/crypto/tresor_asm.S linux-5.4.49-ot/arch/x86/crypto/tresor_asm.S
--- linux-5.4.49-ot.orig/arch/x86/crypto/tresor_asm.S	2020-07-02 19:02:35.101879031 -0700
+++ linux-5.4.49-ot/arch/x86/crypto/tresor_asm.S	2020-07-02 19:16:42.987247758 -0700
@@ -28,6 +28,11 @@
 .set	db2,	%db2	/* round key 1a */
 .set	db3,	%db3	/* round key 1b */
 
+/* 64-bit values storing parts of the tweak key for xts */
+.set	db4,	%db4		/* tweak key 0a */
+.set	db5,	%db5		/* tweak key 0b */
+.set	db6,	%db6		/* tweak key 1a */
+.set	db7,	%db7		/* tweak key 1b */
 
 /* 128-bit SSE registers */
 .set	rstate,	%xmm0		/* AES state */
@@ -221,6 +226,26 @@
 .endm
 
 
+/* copy xts tweak key from dbg regs into xmm regs */
+.macro	read_key_xts_tweak r0 r1 rounds
+	movq	db4,%rax
+	movq	%rax,\r0
+	movq	db5,%rax
+	movq	%rax,rhelp
+	shufps	$0x44,rhelp,\r0
+	.if (\rounds > 10)
+	movq	db6,%rax
+	movq	%rax,\r1
+	.endif
+	.if (\rounds > 12)
+	movq	db7,%rax
+	movq	%rax,rhelp
+	shufps	$0x44,rhelp,\r1
+	.endif
+	xorq	%rax,%rax	/* clear rax, as it contained key material */
+.endm
+
+
 /* Encrypt */
 .macro	encrypt_block rounds
 	movdqu	0(%rsi),rstate
@@ -249,6 +274,34 @@
 .endm
 
 
+/* Encrypt */
+.macro	encrypt_block_xts_tweak rounds
+	movdqu	0(%rsi),rstate
+	read_key_xts_tweak	rk0 rk1 \rounds
+	pxor		rk0,rstate
+	generate_rks_\rounds
+	aesenc		rk1,rstate
+	aesenc		rk2,rstate
+	aesenc		rk3,rstate
+	aesenc		rk4,rstate
+	aesenc		rk5,rstate
+	aesenc		rk6,rstate
+	aesenc		rk7,rstate
+	aesenc		rk8,rstate
+	aesenc		rk9,rstate
+	.if (\rounds > 10)
+	aesenc		rk10,rstate
+	aesenc		rk11,rstate
+	.endif
+	.if (\rounds > 12)
+	aesenc		rk12,rstate
+	aesenc		rk13,rstate
+	.endif
+	aesenclast	rk\rounds,rstate
+	epilog
+.endm
+
+
 /* Decrypt */
 .macro	decrypt_block rounds
 	movdqu	0(%rsi),rstate
@@ -295,6 +348,10 @@
 	.globl	tresor_decblk_192
 	.globl	tresor_encblk_256
 	.globl	tresor_decblk_256
+	.globl	tresor_set_key_xts_tweak
+	.globl	tresor_encblk_128_xts_tweak
+	.globl	tresor_encblk_192_xts_tweak
+	.globl	tresor_encblk_256_xts_tweak
 
 
 /* void tresor_encblk(u8 *out, const u8 *in) */
@@ -306,6 +363,17 @@ tresor_encblk_256:
 	encrypt_block	14
 
 
+/* void tresor_encblk_128_xts_tweak(u8 *out, const u8 *in) */
+tresor_encblk_128:
+	encrypt_block_xts_tweak	10
+/* void tresor_encblk_192_xts_tweak(u8 *out, const u8 *in) */
+tresor_encblk_192:
+	encrypt_block_xts_tweak	12
+/* void tresor_encblk_256_xts_tweak(u8 *out, const u8 *in) */
+tresor_encblk_256:
+	encrypt_block_xts_tweak	14
+
+
 /* void tresor_decblk(u8 *out, const u8 *in) */
 tresor_decblk_128:
 	decrypt_block	10
@@ -328,6 +396,19 @@ tresor_set_key:
 	xorq	%rax,%rax	/* clear rax, as it contained key material */
 	retq
 
+/* void tresor_set_key_xts_tweak(const u8 *in_key) */
+tresor_set_key_xts_tweak:
+	movq	0(%rdi),%rax
+	movq	%rax,db4
+	movq	8(%rdi),%rax
+	movq	%rax,db5
+	movq	16(%rdi),%rax
+	movq	%rax,db6
+	movq	24(%rdi),%rax
+	movq	%rax,db7
+	xorq	%rax,%rax	/* clear rax, as it contained key material */
+	retq
+
 #ifdef CONFIG_CRYPTO_TRESOR_KEY_VIA_CPU0
 /* void tresor_get_key(u8 *out_key) */
 tresor_get_key:
Only in linux-5.4.49-ot/arch/x86/crypto: tresor_asm.S.orig
Only in linux-5.4.49-ot/arch/x86/crypto: tresor_asm.S.rej
diff -urp linux-5.4.49-ot.orig/arch/x86/crypto/tresor_glue.c linux-5.4.49-ot/arch/x86/crypto/tresor_glue.c
--- linux-5.4.49-ot.orig/arch/x86/crypto/tresor_glue.c	2020-07-02 19:02:35.102879070 -0700
+++ linux-5.4.49-ot/arch/x86/crypto/tresor_glue.c	2020-07-02 19:51:58.826339747 -0700
@@ -5,6 +5,26 @@
  * Copyright (C) 2010	Tilo Mueller <tilo.mueller@informatik.uni-erlangen.de>
  * Copyright (C) 2012	Hans Spath <tresor@hans-spath.de>
  *
+ * Portions use skcipher cbc_, ecb_, ctr_ code from crypto/cast5_avx_glue.c by:
+ *   Copyright (C) 2012 Johannes Goetzfried
+ *       <Johannes.Goetzfried@informatik.stud.uni-erlangen.de>
+ *   Copyright © 2013 Jussi Kivilinna <jussi.kivilinna@iki.fi>
+ *
+ * Portions use aesni_init and aesni_skciphers code from crypto/aesni-intel_glue.c by:
+ *   Copyright (C) 2008, Intel Corp.
+ *      Author: Huang Ying <ying.huang@intel.com>
+ *
+ * Portions use xts glue code from crypto/cast6_avx_glue.c
+ *   Copyright (C) 2012 Johannes Goetzfried
+ *       <Johannes.Goetzfried@informatik.stud.uni-erlangen.de>
+ *   Copyright © 2013 Jussi Kivilinna <jussi.kivilinna@iki.fi>
+ *
+ * CBC & ECB parts based on code (crypto/cbc.c,ecb.c) by:
+ *   Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>
+ *
+ * CTR part based on code (crypto/ctr.c) by:
+ *   (C) Copyright IBM Corp. 2007 - Joy Latten <latten@us.ibm.com>
+ *
  * This program is free software; you can redistribute it and/or modify it
  * under the terms and conditions of the GNU General Public License,
  * version 2, as published by the Free Software Foundation.
@@ -17,10 +37,17 @@
  * You should have received a copy of the GNU General Public License along with
  * this program; if not, write to the Free Software Foundation, Inc., 59 Temple
  * Place - Suite 330, Boston, MA 02111-1307 USA.
+ *
+ * Portions include skcipher updates by Eric Biggers <ebiggers@google.com> and
+ * Herbert Xu <herbert@gondor.apana.org.au>.
  */
 
+#include <asm/crypto/glue_helper.h>
 #include <crypto/algapi.h>
+#include <crypto/b128ops.h>
+#include <crypto/internal/simd.h>
 #include <crypto/tresor.h>
+#include <crypto/xts.h>
 #include <linux/module.h>
 #include <crypto/aes.h>
 #include <linux/smp.h>
@@ -31,12 +58,16 @@
  */
 asmlinkage bool tresor_capable(void);
 asmlinkage void tresor_set_key(const u8 *in_key);
+asmlinkage void tresor_set_key_xts_tweak(const u8 *in_key);
 asmlinkage void tresor_encblk_128(u8 *out, const u8 *in);
 asmlinkage void tresor_decblk_128(u8 *out, const u8 *in);
 asmlinkage void tresor_encblk_192(u8 *out, const u8 *in);
 asmlinkage void tresor_decblk_192(u8 *out, const u8 *in);
 asmlinkage void tresor_encblk_256(u8 *out, const u8 *in);
 asmlinkage void tresor_decblk_256(u8 *out, const u8 *in);
+asmlinkage void tresor_encblk_128_xts_tweak(u8 *out, const u8 *in);
+asmlinkage void tresor_encblk_192_xts_tweak(u8 *out, const u8 *in);
+asmlinkage void tresor_encblk_256_xts_tweak(u8 *out, const u8 *in);
 
 
 
@@ -150,6 +181,390 @@ void tresor_setkey(const u8 *in_key)
 	on_each_cpu(tresor_setkey_current_cpu, (void *)in_key, 1);
 }
 
+/*
+ * Set XTS tweak key
+ */
+static void tresor_setkey_xts_tweak_current_cpu(void *data)
+{
+	printk(KERN_DEBUG "TRESOR: %s running on cpu %d\n",
+		__func__, smp_processor_id());
+	tresor_set_key_xts_tweak((const u8 *)data);
+}
+
+void tresor_setkey_xts_tweak(const u8 *in_key)
+{
+	on_each_cpu(tresor_setkey_xts_tweak_current_cpu, (void *)in_key, 1);
+}
+
+static int tresor_skcipher_setkey(struct crypto_skcipher *tfm, const u8 *key, unsigned int keylen)
+{
+	tresor_setdummykey(crypto_skcipher_tfm(tfm), key, keylen);
+	return 0;
+}
+
+static int ecb_crypt(struct skcipher_request *req, bool enc)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);
+	struct skcipher_walk walk;
+	const unsigned int bsize = AES_BLOCK_SIZE;
+	unsigned int nbytes;
+	void (*fn)(u8 *dst, const u8 *src);
+	int err;
+	unsigned long irq_flags;
+
+	err = skcipher_walk_virt(&walk, req, true);
+
+	switch (ctx->key_length) {
+	case AES_KEYSIZE_128:
+		fn = (enc) ? tresor_encblk_128 : tresor_decblk_128;
+		break;
+	case AES_KEYSIZE_192:
+		fn = (enc) ? tresor_encblk_192 : tresor_decblk_192;
+		break;
+	case AES_KEYSIZE_256:
+		fn = (enc) ? tresor_encblk_256 : tresor_decblk_256;
+		break;
+	}
+
+	tresor_prolog(&irq_flags);
+	while ((nbytes = walk.nbytes)) {
+		u8 *wsrc = walk.src.virt.addr;
+		u8 *wdst = walk.dst.virt.addr;
+
+		/* Handle leftovers */
+		do {
+			fn(wdst, wsrc);
+			wsrc += bsize;
+			wdst += bsize;
+			nbytes -= bsize;
+		} while (nbytes >= bsize);
+
+		err = skcipher_walk_done(&walk, nbytes);
+	}
+	tresor_epilog(&irq_flags);
+
+	return err;
+}
+
+static int ecb_encrypt(struct skcipher_request *req)
+{
+	return ecb_crypt(req, true);
+}
+
+static int ecb_decrypt(struct skcipher_request *req)
+{
+	return ecb_crypt(req, false);
+}
+
+
+static int cbc_encrypt(struct skcipher_request *req)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);
+	const unsigned int bsize = AES_BLOCK_SIZE;
+	struct skcipher_walk walk;
+	unsigned int nbytes;
+	int err;
+	void (*fn)(u8 *dst, const u8 *src);
+
+	err = skcipher_walk_virt(&walk, req, false);
+
+	switch (ctx->key_length) {
+	case AES_KEYSIZE_128:
+		fn = tresor_encblk_128;
+		break;
+	case AES_KEYSIZE_192:
+		fn = tresor_encblk_192;
+		break;
+	case AES_KEYSIZE_256:
+		fn = tresor_encblk_256;
+		break;
+	}
+
+	while ((nbytes = walk.nbytes)) {
+		u64 *src = (u64 *)walk.src.virt.addr;
+		u64 *dst = (u64 *)walk.dst.virt.addr;
+		u64 *iv = (u64 *)walk.iv;
+
+		do {
+			*dst = *src ^ *iv;
+			fn((u8 *)dst, (u8 *)dst);
+			iv = dst;
+			src++;
+			dst++;
+			nbytes -= bsize;
+		} while (nbytes >= bsize);
+
+		*(u64 *)walk.iv = *iv;
+		err = skcipher_walk_done(&walk, nbytes);
+	}
+
+	return err;
+}
+
+static unsigned int __cbc_decrypt(struct crypto_aes_ctx *ctx,
+				  struct skcipher_walk *walk)
+{
+	const unsigned int bsize = AES_BLOCK_SIZE;
+	unsigned int nbytes = walk->nbytes;
+	u64 *src = (u64 *)walk->src.virt.addr;
+	u64 *dst = (u64 *)walk->dst.virt.addr;
+	u64 last_iv;
+	void (*fn)(u8 *dst, const u8 *src);
+
+	/* Start of the last block. */
+	src += nbytes / bsize - 1;
+	dst += nbytes / bsize - 1;
+
+	last_iv = *src;
+
+	switch (ctx->key_length) {
+	case AES_KEYSIZE_128:
+		fn = tresor_decblk_128;
+		break;
+	case AES_KEYSIZE_192:
+		fn = tresor_decblk_192;
+		break;
+	case AES_KEYSIZE_256:
+		fn = tresor_decblk_256;
+		break;
+	}
+
+	/* Handle leftovers */
+	for (;;) {
+		fn((u8 *)dst, (u8 *)src);
+
+		nbytes -= bsize;
+		if (nbytes < bsize)
+			break;
+
+		*dst ^= *(src - 1);
+		src -= 1;
+		dst -= 1;
+	}
+
+	*dst ^= *(u64 *)walk->iv;
+	*(u64 *)walk->iv = last_iv;
+
+	return nbytes;
+}
+
+static int cbc_decrypt(struct skcipher_request *req)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);
+	struct skcipher_walk walk;
+	unsigned int nbytes;
+	int err;
+	unsigned long irq_flags;
+
+	err = skcipher_walk_virt(&walk, req, false);
+
+	tresor_prolog(&irq_flags);
+	while ((nbytes = walk.nbytes)) {
+		nbytes = __cbc_decrypt(ctx, &walk);
+		err = skcipher_walk_done(&walk, nbytes);
+	}
+	tresor_epilog(&irq_flags);
+	return err;
+}
+
+static void ctr_crypt_final(struct skcipher_walk *walk, struct crypto_aes_ctx *ctx)
+{
+	u8 *ctrblk = walk->iv;
+	u8 keystream[AES_BLOCK_SIZE];
+	u8 *src = walk->src.virt.addr;
+	u8 *dst = walk->dst.virt.addr;
+	unsigned int nbytes = walk->nbytes;
+
+	switch (ctx->key_length) {
+	case AES_KEYSIZE_128:
+		tresor_encblk_128(keystream, ctrblk);
+		break;
+	case AES_KEYSIZE_192:
+		tresor_encblk_192(keystream, ctrblk);
+		break;
+	case AES_KEYSIZE_256:
+		tresor_encblk_256(keystream, ctrblk);
+		break;
+	}
+	crypto_xor_cpy(dst, keystream, src, nbytes);
+
+	crypto_inc(ctrblk, AES_BLOCK_SIZE);
+}
+
+static unsigned int __ctr_crypt(struct skcipher_walk *walk,
+				struct crypto_aes_ctx *ctx)
+{
+	const unsigned int bsize = AES_BLOCK_SIZE;
+	unsigned int nbytes = walk->nbytes;
+	u64 *src = (u64 *)walk->src.virt.addr;
+	u64 *dst = (u64 *)walk->dst.virt.addr;
+	void (*fn)(u8 *dst, const u8 *src);
+
+	switch (ctx->key_length) {
+	case AES_KEYSIZE_128:
+		fn = tresor_encblk_128;
+		break;
+	case AES_KEYSIZE_192:
+		fn = tresor_encblk_192;
+		break;
+	case AES_KEYSIZE_256:
+		fn = tresor_encblk_256;
+		break;
+	}
+
+	/* Handle leftovers */
+	do {
+		u64 ctrblk;
+
+		if (dst != src)
+			*dst = *src;
+
+		ctrblk = *(u64 *)walk->iv;
+		be64_add_cpu((__be64 *)walk->iv, 1);
+
+		fn((u8 *)&ctrblk, (u8 *)&ctrblk);
+		*dst ^= ctrblk;
+
+		src += 1;
+		dst += 1;
+		nbytes -= bsize;
+	} while (nbytes >= bsize);
+
+	return nbytes;
+}
+
+static int ctr_crypt(struct skcipher_request *req)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);
+	struct skcipher_walk walk;
+	unsigned int nbytes;
+	int err;
+	unsigned long irq_flags;
+
+	err = skcipher_walk_virt(&walk, req, false);
+
+	tresor_prolog(&irq_flags);
+	while ((nbytes = walk.nbytes) >= AES_BLOCK_SIZE) {
+		nbytes = __ctr_crypt(&walk, ctx);
+		err = skcipher_walk_done(&walk, nbytes);
+	}
+	tresor_epilog(&irq_flags);
+
+	if (walk.nbytes) {
+		ctr_crypt_final(&walk, ctx);
+		err = skcipher_walk_done(&walk, 0);
+	}
+
+	return err;
+}
+
+void __tresor_encrypt(struct tresor_ctx *ctx, u8 *dst, const u8 *src) {
+	unsigned long irq_flags;
+
+	/* encrypt using the cipher key */
+	tresor_prolog(&irq_flags);
+	tresor_encblk_128(dst, src);
+	tresor_epilog(&irq_flags);
+}
+
+void __tresor_decrypt(struct tresor_ctx *ctx, u8 *dst, const u8 *src) {
+	unsigned long irq_flags;
+
+	/* decrypt using the cipher key */
+	tresor_prolog(&irq_flags);
+	tresor_decblk_128(dst, src);
+	tresor_epilog(&irq_flags);
+}
+
+void __xts_tweak_tresor_encrypt(struct tresor_ctx *ctx, u8 *dst, const u8 *src) {
+	unsigned long irq_flags;
+
+	/* encrypt using the tweak key */
+	tresor_prolog(&irq_flags);
+	tresor_encblk_128_xts_tweak(dst, src);
+	tresor_epilog(&irq_flags);
+}
+
+struct tresor_xts_ctx {
+	struct tresor_ctx crypt_ctx;
+	struct tresor_ctx tweak_ctx;
+};
+
+static int xts_tresor_setkey(struct crypto_skcipher *tfm, const u8 *key,
+			    unsigned int keylen)
+{
+	int err;
+
+	err = xts_verify_key(tfm, key, keylen);
+	if (err)
+		return err;
+
+	keylen /= 2;
+
+	if (keylen != AES_KEYSIZE_128
+		&& keylen != AES_KEYSIZE_192
+		&& keylen != AES_KEYSIZE_256)
+		return -EINVAL;
+
+	/* Same reason explained in tresor_setdummykey comment */
+	return 0;
+}
+
+static void tresor_xts_enc(void *ctx, u128 *dst, const u128 *src, le128 *iv)
+{
+	glue_xts_crypt_128bit_one(ctx, dst, src, iv,
+				  GLUE_FUNC_CAST(__tresor_encrypt));
+}
+
+static void tresor_xts_dec(void *ctx, u128 *dst, const u128 *src, le128 *iv)
+{
+	glue_xts_crypt_128bit_one(ctx, dst, src, iv,
+				  GLUE_FUNC_CAST(__tresor_decrypt));
+}
+
+static const struct common_glue_ctx tresor_enc_xts = {
+	.num_funcs = 1,
+	.fpu_blocks_limit = 1,
+
+	.funcs = { {
+		.num_blocks = 1,
+		.fn_u = { .xts = GLUE_XTS_FUNC_CAST(tresor_xts_enc) }
+	} }
+};
+
+static const struct common_glue_ctx tresor_dec_xts = {
+	.num_funcs = 1,
+	.fpu_blocks_limit = 1,
+
+	.funcs = { {
+		.num_blocks = 1,
+		.fn_u = { .xts = GLUE_XTS_FUNC_CAST(tresor_xts_dec) }
+	} }
+};
+
+static int xts_encrypt(struct skcipher_request *req)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct tresor_xts_ctx *ctx = crypto_skcipher_ctx(tfm);
+
+	return glue_xts_req_128bit(&tresor_enc_xts, req,
+				   XTS_TWEAK_CAST(__xts_tweak_tresor_encrypt),
+				   &ctx->tweak_ctx, &ctx->crypt_ctx, false);
+}
+
+static int xts_decrypt(struct skcipher_request *req)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct tresor_xts_ctx *ctx = crypto_skcipher_ctx(tfm);
+
+	return glue_xts_req_128bit(&tresor_dec_xts, req,
+				   XTS_TWEAK_CAST(__xts_tweak_tresor_encrypt),
+				   &ctx->tweak_ctx, &ctx->crypt_ctx, true);
+}
 
 /*
  * Crypto API algorithm
@@ -175,12 +590,96 @@ static struct crypto_alg tresor_alg = {
 	}
 };
 
+static struct skcipher_alg tresor_skciphers[] = {
+	{
+		.base = {
+			.cra_name		= "__ecb(tresor)",
+			.cra_driver_name	= "__ecb-tresor-aesni",
+			.cra_priority		= 400,
+			.cra_flags		= CRYPTO_ALG_INTERNAL,
+			.cra_blocksize		= AES_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct crypto_aes_ctx),
+			.cra_module		= THIS_MODULE,
+		},
+		.min_keysize	= AES_MIN_KEY_SIZE,
+		.max_keysize	= AES_MAX_KEY_SIZE,
+		.setkey		= tresor_skcipher_setkey,
+		.encrypt	= ecb_encrypt,
+		.decrypt	= ecb_decrypt,
+	}, {
+		.base = {
+			.cra_name		= "__cbc(tresor)",
+			.cra_driver_name	= "__cbc-tresor-aesni",
+			.cra_priority		= 400,
+			.cra_flags		= CRYPTO_ALG_INTERNAL,
+			.cra_blocksize		= AES_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct crypto_aes_ctx),
+			.cra_module		= THIS_MODULE,
+		},
+		.min_keysize	= AES_MIN_KEY_SIZE,
+		.max_keysize	= AES_MAX_KEY_SIZE,
+		.ivsize		= AES_BLOCK_SIZE,
+		.setkey		= tresor_skcipher_setkey,
+		.encrypt	= cbc_encrypt,
+		.decrypt	= cbc_decrypt,
+#ifdef CONFIG_X86_64
+	}, {
+		.base = {
+			.cra_name		= "__ctr(tresor)",
+			.cra_driver_name	= "__ctr-tresor-aesni",
+			.cra_priority		= 400,
+			.cra_flags		= CRYPTO_ALG_INTERNAL,
+			.cra_blocksize		= 1,
+			.cra_ctxsize		= sizeof(struct crypto_aes_ctx),
+			.cra_module		= THIS_MODULE,
+		},
+		.min_keysize	= AES_MIN_KEY_SIZE,
+		.max_keysize	= AES_MAX_KEY_SIZE,
+		.ivsize		= AES_BLOCK_SIZE,
+		.chunksize	= AES_BLOCK_SIZE,
+		.setkey		= tresor_skcipher_setkey,
+		.encrypt	= ctr_crypt,
+		.decrypt	= ctr_crypt,
+	}, {
+		.base = {
+			.cra_name		= "__xts(aes)",
+			.cra_driver_name	= "__xts-tresor-aesni",
+			.cra_priority		= 401,
+			.cra_flags		= CRYPTO_ALG_INTERNAL,
+			.cra_blocksize		= AES_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct tresor_xts_ctx),
+			.cra_module		= THIS_MODULE,
+		},
+		.min_keysize	= 2 * AES_MIN_KEY_SIZE,
+		.max_keysize	= 2 * AES_MAX_KEY_SIZE,
+		.ivsize		= AES_BLOCK_SIZE,
+		.setkey		= xts_tresor_setkey,
+		.encrypt	= xts_encrypt,
+		.decrypt	= xts_decrypt,
+#endif
+	}
+};
+
+static struct simd_skcipher_alg *tresor_simd_skciphers[ARRAY_SIZE(tresor_skciphers)];
 
 /* Initialize module */
 static int __init tresor_init(void)
 {
 	int retval;
 	retval = crypto_register_alg(&tresor_alg);
+	if (retval)
+		return retval;
+
+	retval = simd_register_skciphers_compat(tresor_skciphers,
+					     ARRAY_SIZE(tresor_skciphers),
+					     tresor_simd_skciphers);
+	if (retval)
+		goto unregister_cipher;
+
+	return 0;
+
+unregister_cipher:
+	crypto_unregister_alg(&tresor_alg);
 	return retval;
 }
 module_init(tresor_init);
@@ -189,6 +688,8 @@ module_init(tresor_init);
 /* Remove module */
 static void __exit tresor_fini(void)
 {
+	simd_unregister_skciphers(tresor_skciphers, ARRAY_SIZE(tresor_skciphers),
+				  tresor_simd_skciphers);
 	crypto_unregister_alg(&tresor_alg);
 }
 module_exit(tresor_fini);
Only in linux-5.4.49-ot/arch/x86/crypto: tresor_glue.c.orig
Only in linux-5.4.49-ot/arch/x86/crypto: tresor_glue.c.rej
diff -urp linux-5.4.49-ot.orig/crypto/testmgr.c linux-5.4.49-ot/crypto/testmgr.c
--- linux-5.4.49-ot.orig/crypto/testmgr.c	2020-07-02 19:02:35.171881789 -0700
+++ linux-5.4.49-ot/crypto/testmgr.c	2020-07-02 19:05:53.305670298 -0700
@@ -2500,8 +2500,11 @@ static int test_skcipher_vec_cfg(const c
 		crypto_skcipher_clear_flags(tfm,
 					    CRYPTO_TFM_REQ_FORBID_WEAK_KEYS);
 #ifdef CONFIG_CRYPTO_TRESOR
-	if (strstr(driver, "tresor"))
+	if (strstr(driver, "tresor")) {
 		tresor_setkey(vec->key);
+		if (strstr(driver, "xts"))
+			tresor_setkey_xts_tweak(vec->key + tfm->keysize/2);
+	}
 #endif
 	err = crypto_skcipher_setkey(tfm, vec->key, vec->klen);
 	if (err) {
diff -urp linux-5.4.49-ot.orig/include/crypto/tresor.h linux-5.4.49-ot/include/crypto/tresor.h
--- linux-5.4.49-ot.orig/include/crypto/tresor.h	2020-07-02 19:02:35.111879425 -0700
+++ linux-5.4.49-ot/include/crypto/tresor.h	2020-07-02 19:05:53.305670298 -0700
@@ -10,10 +10,15 @@
 /* number of chars to clear memory */
 #define TRESOR_RANDOM_CHARS 4096
 
+struct tresor_ctx {
+	int dummy;
+};
+
 /* TRESOR core functionality (enc, dec, setkey) */
 void tresor_encrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src);
 void tresor_decrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src);
 void tresor_setkey(const u8 *in_key);
+void tresor_setkey_xts_tweak(const u8 *in_key);
 bool tresor_capable(void);
 
 void tresor_kernel_init(void);
Only in linux-5.4.49-ot/include/crypto: tresor.h.orig
