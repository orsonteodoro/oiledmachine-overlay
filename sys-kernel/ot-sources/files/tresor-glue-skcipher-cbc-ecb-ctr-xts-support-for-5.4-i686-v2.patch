Patch Status: Testing
Patch Author: Orson Teodoro <orsonteodoro@hotmail.com>
Date: Jul 3, 2020
v2:  Move tweak key from db4-db7 to high order db0-db3.
     Only allow XTS in 64-bit.
     Changes for key_length.
     Updated testmgr.c for ctr(tresor), xts(tresor).
     Update init/main.c to run ctr(tresor), xts(tresor) tests.
     Fixed .cra_name for xts(tresor) that prevented opening dm-crypt devices.
     Restore registers after using tresor_set_key_xts_tweak_128.
     Added suffixes to assembly instructions.
v1:  initial release
TODO
Check if tresor_setkey_xts_tweak is required for crypto_skcipher_setkey
  consumers (See crypto/testmgr.c for details) for applications / 
  crypto-drivers / crypto-libraries
----
diff -urp linux-5.4.50-ot.orig/arch/x86/crypto/tresor_asm.S linux-5.4.50-ot/arch/x86/crypto/tresor_asm.S
--- linux-5.4.50-ot.orig/arch/x86/crypto/tresor_asm.S	2020-07-03 22:01:03.584206477 -0700
+++ linux-5.4.50-ot/arch/x86/crypto/tresor_asm.S	2020-07-03 22:03:13.371991659 -0700
@@ -42,11 +42,13 @@
 .set	rk9a,	%mm6		/* round key 9a */
 .set	rk9b,	%mm7		/* round key 9b */
 
-/* 32-bit ^ 64-bit debug registers but with 32-bit values */
-.set	db0,	%db0		/* round key 0a */
-.set	db1,	%db1		/* round key 0b */
-.set	db2,	%db2		/* round key 1a */
-.set	db3,	%db3		/* round key 1b */
+/* 32-bit ^ 64-bit debug registers */
+/* low 32-bit values are for the crypto key */
+/* high 32-bit values are for the tweak key for xts */
+.set	db0,	%db0		/* hi(tweak key 0a) lo(round key 0a) */
+.set	db1,	%db1		/* hi(tweak key 0b) lo(round key 0b) */
+.set	db2,	%db2		/* hi(tweak key 1a) lo(round key 1a) */
+.set	db3,	%db3		/* hi(tweak key 1b) lo(round key 1b) */
 
 /* 32-bit GPR registers */
 .set	eax,	%eax
@@ -278,6 +280,36 @@ rc_tab:	.long	0x00000001, 0x00000002, 0x
 	pxor		rhelp,\sse
 .endm
 
+/* copy four 32-bit values from debug registers into one 128-bit sse register */
+.macro	xts_dbg_to_sse dbg0 dbg1 dbg2 dbg3 sse
+#if __x86_64__
+	movq		\dbg0,rax
+	rorq		$32,rax
+	movd		eax,\sse
+	movq		\dbg1,rax
+	rorq		$32,rax
+	movd		eax,rhelp
+	pslldq		$4,rhelp
+	pxor		rhelp,\sse
+	movq		\dbg2,rax
+	rorq		$32,rax
+	movd		eax,rhelp
+	pslldq		$8,rhelp
+	pxor		rhelp,\sse
+	movq		\dbg3,rax
+	rorq		$32,rax
+#else
+#endif
+	movd		eax,rhelp
+	pslldq		$12,rhelp
+	pxor		rhelp,\sse
+#if __x86_64__
+	xorq		rax,rax
+#else
+	xorl		eax,eax
+#endif
+.endm
+
 /* copy four 32-bit general purpose registers into one 128-bit sse register */
 .macro	gpr_to_sse gpr0 gpr1 gpr2 gpr3 sse
 	movd		\gpr0,\sse
@@ -518,6 +550,8 @@ rc_tab:	.long	0x00000001, 0x00000002, 0x
 	.global		tresor_set_key
 	.global		tresor_encblk_128
 	.global		tresor_decblk_128
+	.global		tresor_set_key_xts_tweak_128
+	.global		tresor_encblk_128_xts_tweak
 	.extern		crypto_ft_tab
 	.extern		crypto_fl_tab
 	.extern		crypto_it_tab
@@ -560,6 +594,24 @@ tresor_decblk_128:
 	i_lround	rk1
 	epilog
 
+/* void tresor_encblk_128_xts_tweak(u8 *out, const u8 *in) */
+tresor_encblk_128_xts_tweak:
+	prolog 
+	xts_dbg_to_sse	db0,db1,db2,db3,rk1
+	pxor		rk1,rstate
+	key_schedule
+	f_nround	rk1
+	f_nround	rk2
+	f_nround	rk3
+	f_nround	rk4
+	f_nround	rk5
+	f_nround_	rk6a,rk6b,rk5
+	f_nround_	rk7a,rk7b,rk5
+	f_nround_	rk8a,rk8b,rk5
+	f_nround_	rk9a,rk9b,rk5
+	f_lround	rk10
+	epilog
+
 /* void tresor_set_key(const u8 *in_key) */
 tresor_set_key:
 #if __x86_64__
@@ -584,3 +636,49 @@ tresor_set_key:
 #endif
 	xorl		eax, eax
 	ret
+
+/* void tresor_set_key_xts_tweak_128(const u8 *in_key) */
+tresor_set_key_xts_tweak_128:
+#if __x86_64__
+	pushq		rcx
+	pushq		rbx
+	movl		0(rdi),eax
+	movq		$0x00000000FFFFFFFF,rcx
+	andq		rcx,rax
+	rolq		$32,rax
+	movq		db0,rbx
+	andq		rcx,rbx
+	orq		rax,rbx
+	movq		rbx,db0
+
+	movl		4(rdi),eax
+	andq		rcx,rax
+	rolq		$32,rax
+	movq		db1,rbx
+	andq		rcx,rbx
+	orq		rax,rbx
+	movq		rbx,db1
+
+	movl		8(rdi),eax
+	andq		rcx,rax
+	rolq		$32,rax
+	movq		db2,rbx
+	andq		rcx,rbx
+	orq		rax,rbx
+	movq		rbx,db2
+
+	movl		12(rdi),eax
+	andq		rcx,rax
+	rolq		$32,rax
+	movq		db3,rbx
+	andq		rcx,rbx
+	orq		rax,rbx
+	movq		rbx,db3
+
+	xorq		rax,rax
+	popq		rbx
+	popq		rcx
+#else
+#endif
+	xorl		eax,eax
+	ret
diff -urp linux-5.4.50-ot.orig/arch/x86/crypto/tresor_glue.c linux-5.4.50-ot/arch/x86/crypto/tresor_glue.c
--- linux-5.4.50-ot.orig/arch/x86/crypto/tresor_glue.c	2020-07-03 22:01:03.542204952 -0700
+++ linux-5.4.50-ot/arch/x86/crypto/tresor_glue.c	2020-07-03 22:03:13.372991696 -0700
@@ -5,6 +5,26 @@
  * Copyright (C) 2012	Hans Spath <tresor@hans-spath.de>
  * Copyright (C) 2012	Johannes Goetzfried <johannes@jgoetzfried.de>
  *
+ * Portions use skcipher cbc_, ecb_, ctr_ code from crypto/cast5_avx_glue.c by:
+ *   Copyright (C) 2012 Johannes Goetzfried
+ *       <Johannes.Goetzfried@informatik.stud.uni-erlangen.de>
+ *   Copyright © 2013 Jussi Kivilinna <jussi.kivilinna@iki.fi>
+ *
+ * Portions use aesni_init and aesni_skciphers code from crypto/aesni-intel_glue.c by:
+ *   Copyright (C) 2008, Intel Corp.
+ *      Author: Huang Ying <ying.huang@intel.com>
+ *
+ * Portions use xts glue code from crypto/cast6_avx_glue.c
+ *   Copyright (C) 2012 Johannes Goetzfried
+ *       <Johannes.Goetzfried@informatik.stud.uni-erlangen.de>
+ *   Copyright © 2013 Jussi Kivilinna <jussi.kivilinna@iki.fi>
+ *
+ * CBC & ECB parts based on code (crypto/cbc.c,ecb.c) by:
+ *   Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>
+ *
+ * CTR part based on code (crypto/ctr.c) by:
+ *   (C) Copyright IBM Corp. 2007 - Joy Latten <latten@us.ibm.com>
+ *
  * This program is free software; you can redistribute it and/or modify it
  * under the terms and conditions of the GNU General Public License,
  * version 2, as published by the Free Software Foundation.
@@ -17,10 +37,17 @@
  * You should have received a copy of the GNU General Public License along with
  * this program; if not, write to the Free Software Foundation, Inc., 59 Temple
  * Place - Suite 330, Boston, MA 02111-1307 USA.
+ *
+ * Portions include skcipher updates by Eric Biggers <ebiggers@google.com> and
+ * Herbert Xu <herbert@gondor.apana.org.au>.
  */
 
+#include <asm/crypto/glue_helper.h>
 #include <crypto/algapi.h>
+#include <crypto/b128ops.h>
+#include <crypto/internal/simd.h>
 #include <crypto/tresor.h>
+#include <crypto/xts.h>
 #include <linux/module.h>
 #include <crypto/aes.h>
 #include <linux/smp.h>
@@ -30,8 +57,10 @@
  * Assembly functions implemented in tresor-intel_asm.S
  */
 asmlinkage void tresor_set_key(const u8 *in_key);
+asmlinkage void tresor_set_key_xts_tweak_128(const u8 *in_key);
 asmlinkage void tresor_encblk_128(u8 *out, const u8 *in);
 asmlinkage void tresor_decblk_128(u8 *out, const u8 *in);
+asmlinkage void tresor_encblk_128_xts_tweak(u8 *out, const u8 *in);
 
 
 
@@ -117,6 +146,336 @@ void tresor_setkey(const u8 *in_key)
 	on_each_cpu(tresor_setkey_current_cpu, (void *)in_key, 1);
 }
 
+/*
+ * Set XTS tweak key
+ */
+static void tresor_setkey_xts_tweak_current_cpu(void *data)
+{
+	printk(KERN_DEBUG "TRESOR: %s running on cpu %d\n",
+		__func__, smp_processor_id());
+	tresor_set_key_xts_tweak_128((const u8 *)data);
+}
+
+void tresor_setkey_xts_tweak(const u8 *in_key)
+{
+	on_each_cpu(tresor_setkey_xts_tweak_current_cpu, (void *)in_key, 1);
+}
+
+static int tresor_skcipher_setkey(struct crypto_skcipher *tfm, const u8 *key, unsigned int keylen)
+{
+	tresor_setdummykey(crypto_skcipher_tfm(tfm), key, keylen);
+	return 0;
+}
+
+static int ecb_crypt(struct skcipher_request *req, bool enc)
+{
+	struct skcipher_walk walk;
+	const unsigned int bsize = AES_BLOCK_SIZE;
+	unsigned int nbytes;
+	void (*fn)(u8 *dst, const u8 *src);
+	int err;
+	unsigned long irq_flags;
+
+	err = skcipher_walk_virt(&walk, req, true);
+
+	fn = (enc) ? tresor_encblk_128 : tresor_decblk_128;
+
+	tresor_prolog(&irq_flags);
+	while ((nbytes = walk.nbytes)) {
+		u8 *wsrc = walk.src.virt.addr;
+		u8 *wdst = walk.dst.virt.addr;
+
+		fn = (enc) ? tresor_encblk_128 : tresor_decblk_128;
+
+		/* Handle leftovers */
+		do {
+			fn(wdst, wsrc);
+
+			wsrc += bsize;
+			wdst += bsize;
+			nbytes -= bsize;
+		} while (nbytes >= bsize);
+
+		err = skcipher_walk_done(&walk, nbytes);
+	}
+	tresor_epilog(&irq_flags);
+
+	return err;
+}
+
+static int ecb_encrypt(struct skcipher_request *req)
+{
+	return ecb_crypt(req, true);
+}
+
+static int ecb_decrypt(struct skcipher_request *req)
+{
+	return ecb_crypt(req, false);
+}
+
+
+static int cbc_encrypt(struct skcipher_request *req)
+{
+	const unsigned int bsize = AES_BLOCK_SIZE;
+	struct skcipher_walk walk;
+	unsigned int nbytes;
+	int err;
+
+	err = skcipher_walk_virt(&walk, req, false);
+
+	while ((nbytes = walk.nbytes)) {
+		u64 *src = (u64 *)walk.src.virt.addr;
+		u64 *dst = (u64 *)walk.dst.virt.addr;
+		u64 *iv = (u64 *)walk.iv;
+
+		do {
+			*dst = *src ^ *iv;
+			tresor_encblk_128((u8 *)dst, (u8 *)dst);
+			iv = dst;
+			src++;
+			dst++;
+			nbytes -= bsize;
+		} while (nbytes >= bsize);
+
+		*(u64 *)walk.iv = *iv;
+		err = skcipher_walk_done(&walk, nbytes);
+	}
+
+	return err;
+}
+
+static unsigned int __cbc_decrypt(struct crypto_aes_ctx *ctx,
+				  struct skcipher_walk *walk)
+{
+	const unsigned int bsize = AES_BLOCK_SIZE;
+	unsigned int nbytes = walk->nbytes;
+	u64 *src = (u64 *)walk->src.virt.addr;
+	u64 *dst = (u64 *)walk->dst.virt.addr;
+	u64 last_iv;
+
+	/* Start of the last block. */
+	src += nbytes / bsize - 1;
+	dst += nbytes / bsize - 1;
+
+	last_iv = *src;
+
+	/* Handle leftovers */
+	for (;;) {
+		tresor_decblk_128((u8 *)dst, (u8 *)src);
+
+		nbytes -= bsize;
+		if (nbytes < bsize)
+			break;
+
+		*dst ^= *(src - 1);
+		src -= 1;
+		dst -= 1;
+	}
+
+	*dst ^= *(u64 *)walk->iv;
+	*(u64 *)walk->iv = last_iv;
+
+	return nbytes;
+}
+
+static int cbc_decrypt(struct skcipher_request *req)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);
+	struct skcipher_walk walk;
+	unsigned int nbytes;
+	int err;
+	unsigned long irq_flags;
+
+	err = skcipher_walk_virt(&walk, req, false);
+
+	tresor_prolog(&irq_flags);
+	while ((nbytes = walk.nbytes)) {
+		nbytes = __cbc_decrypt(ctx, &walk);
+		err = skcipher_walk_done(&walk, nbytes);
+	}
+	tresor_epilog(&irq_flags);
+	return err;
+}
+
+static void ctr_crypt_final(struct skcipher_walk *walk, struct crypto_aes_ctx *ctx)
+{
+	u8 *ctrblk = walk->iv;
+	u8 keystream[AES_BLOCK_SIZE];
+	u8 *src = walk->src.virt.addr;
+	u8 *dst = walk->dst.virt.addr;
+	unsigned int nbytes = walk->nbytes;
+
+	tresor_encblk_128(keystream, ctrblk);
+	crypto_xor_cpy(dst, keystream, src, nbytes);
+
+	crypto_inc(ctrblk, AES_BLOCK_SIZE);
+}
+
+static unsigned int __ctr_crypt(struct skcipher_walk *walk,
+				struct crypto_aes_ctx *ctx)
+{
+	const unsigned int bsize = AES_BLOCK_SIZE;
+	unsigned int nbytes = walk->nbytes;
+	u64 *src = (u64 *)walk->src.virt.addr;
+	u64 *dst = (u64 *)walk->dst.virt.addr;
+
+	/* Handle leftovers */
+	do {
+		u64 ctrblk;
+
+		if (dst != src)
+			*dst = *src;
+
+		ctrblk = *(u64 *)walk->iv;
+		be64_add_cpu((__be64 *)walk->iv, 1);
+
+		tresor_encblk_128((u8 *)&ctrblk, (u8 *)&ctrblk);
+		*dst ^= ctrblk;
+
+		src += 1;
+		dst += 1;
+		nbytes -= bsize;
+	} while (nbytes >= bsize);
+
+	return nbytes;
+}
+
+static int ctr_crypt(struct skcipher_request *req)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);
+	struct skcipher_walk walk;
+	unsigned int nbytes;
+	int err;
+	unsigned long irq_flags;
+
+	err = skcipher_walk_virt(&walk, req, false);
+
+	tresor_prolog(&irq_flags);
+	while ((nbytes = walk.nbytes) >= AES_BLOCK_SIZE) {
+		nbytes = __ctr_crypt(&walk, ctx);
+		err = skcipher_walk_done(&walk, nbytes);
+	}
+	tresor_epilog(&irq_flags);
+
+	if (walk.nbytes) {
+		ctr_crypt_final(&walk, ctx);
+		err = skcipher_walk_done(&walk, 0);
+	}
+
+	return err;
+}
+
+#ifdef CONFIG_X86_64
+/* 32-bit doesn't support 64-bit hardware breakpoint addresses */
+void __tresor_encrypt(struct tresor_ctx *ctx, u8 *dst, const u8 *src) {
+	unsigned long irq_flags;
+
+	/* encrypt using the cipher key */
+	tresor_prolog(&irq_flags);
+	tresor_encblk_128(dst, src);
+	tresor_epilog(&irq_flags);
+}
+
+void __tresor_decrypt(struct tresor_ctx *ctx, u8 *dst, const u8 *src) {
+	unsigned long irq_flags;
+
+	/* decrypt using the cipher key */
+	tresor_prolog(&irq_flags);
+	tresor_decblk_128(dst, src);
+	tresor_epilog(&irq_flags);
+}
+
+void __xts_tweak_tresor_encrypt(struct tresor_ctx *ctx, u8 *dst, const u8 *src) {
+	unsigned long irq_flags;
+
+	/* encrypt using the tweak key */
+	tresor_prolog(&irq_flags);
+	tresor_encblk_128_xts_tweak(dst, src);
+	tresor_epilog(&irq_flags);
+}
+
+struct tresor_xts_ctx {
+	struct tresor_ctx crypt_ctx;
+	struct tresor_ctx tweak_ctx;
+};
+
+static int xts_tresor_setkey(struct crypto_skcipher *tfm, const u8 *key,
+			    unsigned int keylen)
+{
+	struct tresor_xts_ctx *ctx = crypto_skcipher_ctx(tfm);
+	int err;
+
+	keylen /= 2;
+
+	if (keylen != AES_KEYSIZE_128)
+		return -EINVAL;
+
+
+	err = xts_verify_key(tfm, key, keylen*2);
+	if (err)
+		return err;
+
+	ctx->crypt_ctx.key_length = keylen;
+	ctx->tweak_ctx.key_length = keylen;
+
+	/* Same reason explained in tresor_setdummykey comment */
+	return 0;
+}
+
+static void tresor_xts_enc(void *ctx, u128 *dst, const u128 *src, le128 *iv)
+{
+	glue_xts_crypt_128bit_one(ctx, dst, src, iv,
+				  GLUE_FUNC_CAST(__tresor_encrypt));
+}
+
+static void tresor_xts_dec(void *ctx, u128 *dst, const u128 *src, le128 *iv)
+{
+	glue_xts_crypt_128bit_one(ctx, dst, src, iv,
+				  GLUE_FUNC_CAST(__tresor_decrypt));
+}
+
+static const struct common_glue_ctx tresor_enc_xts = {
+	.num_funcs = 1,
+	.fpu_blocks_limit = 1,
+
+	.funcs = { {
+		.num_blocks = 1,
+		.fn_u = { .xts = GLUE_XTS_FUNC_CAST(tresor_xts_enc) }
+	} }
+};
+
+static const struct common_glue_ctx tresor_dec_xts = {
+	.num_funcs = 1,
+	.fpu_blocks_limit = 1,
+
+	.funcs = { {
+		.num_blocks = 1,
+		.fn_u = { .xts = GLUE_XTS_FUNC_CAST(tresor_xts_dec) }
+	} }
+};
+
+static int xts_encrypt(struct skcipher_request *req)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct tresor_xts_ctx *ctx = crypto_skcipher_ctx(tfm);
+
+	return glue_xts_req_128bit(&tresor_enc_xts, req,
+				   XTS_TWEAK_CAST(__xts_tweak_tresor_encrypt),
+				   &ctx->tweak_ctx, &ctx->crypt_ctx, false);
+}
+
+static int xts_decrypt(struct skcipher_request *req)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct tresor_xts_ctx *ctx = crypto_skcipher_ctx(tfm);
+
+	return glue_xts_req_128bit(&tresor_dec_xts, req,
+				   XTS_TWEAK_CAST(__xts_tweak_tresor_encrypt),
+				   &ctx->tweak_ctx, &ctx->crypt_ctx, true);
+}
+#endif
 
 /*
  * Crypto API algorithm
@@ -142,12 +501,96 @@ static struct crypto_alg tresor_alg = {
 	}
 };
 
+static struct skcipher_alg tresor_skciphers[] = {
+	{
+		.base = {
+			.cra_name		= "__ecb(tresor)",
+			.cra_driver_name	= "__ecb-tresor-sse2",
+			.cra_priority		= 400,
+			.cra_flags		= CRYPTO_ALG_INTERNAL,
+			.cra_blocksize		= AES_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct crypto_aes_ctx),
+			.cra_module		= THIS_MODULE,
+		},
+		.min_keysize	= AES_MIN_KEY_SIZE,
+		.max_keysize	= AES_MIN_KEY_SIZE,
+		.setkey		= tresor_skcipher_setkey,
+		.encrypt	= ecb_encrypt,
+		.decrypt	= ecb_decrypt,
+	}, {
+		.base = {
+			.cra_name		= "__cbc(tresor)",
+			.cra_driver_name	= "__cbc-tresor-sse2",
+			.cra_priority		= 400,
+			.cra_flags		= CRYPTO_ALG_INTERNAL,
+			.cra_blocksize		= AES_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct crypto_aes_ctx),
+			.cra_module		= THIS_MODULE,
+		},
+		.min_keysize	= AES_MIN_KEY_SIZE,
+		.max_keysize	= AES_MIN_KEY_SIZE,
+		.ivsize		= AES_BLOCK_SIZE,
+		.setkey		= tresor_skcipher_setkey,
+		.encrypt	= cbc_encrypt,
+		.decrypt	= cbc_decrypt,
+#ifdef CONFIG_X86_64
+	}, {
+		.base = {
+			.cra_name		= "__ctr(tresor)",
+			.cra_driver_name	= "__ctr-tresor-sse2",
+			.cra_priority		= 400,
+			.cra_flags		= CRYPTO_ALG_INTERNAL,
+			.cra_blocksize		= 1,
+			.cra_ctxsize		= sizeof(struct crypto_aes_ctx),
+			.cra_module		= THIS_MODULE,
+		},
+		.min_keysize	= AES_MIN_KEY_SIZE,
+		.max_keysize	= AES_MIN_KEY_SIZE,
+		.ivsize		= AES_BLOCK_SIZE,
+		.chunksize	= AES_BLOCK_SIZE,
+		.setkey		= tresor_skcipher_setkey,
+		.encrypt	= ctr_crypt,
+		.decrypt	= ctr_crypt,
+	}, {
+		.base = {
+			.cra_name		= "__xts(tresor)",
+			.cra_driver_name	= "__xts-tresor-sse2",
+			.cra_priority		= 401,
+			.cra_flags		= CRYPTO_ALG_INTERNAL,
+			.cra_blocksize		= AES_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct tresor_xts_ctx),
+			.cra_module		= THIS_MODULE,
+		},
+		.min_keysize	= 2 * AES_MIN_KEY_SIZE,
+		.max_keysize	= 2 * AES_MIN_KEY_SIZE,
+		.ivsize		= AES_BLOCK_SIZE,
+		.setkey		= xts_tresor_setkey,
+		.encrypt	= xts_encrypt,
+		.decrypt	= xts_decrypt,
+#endif
+	}
+};
+
+static struct simd_skcipher_alg *tresor_simd_skciphers[ARRAY_SIZE(tresor_skciphers)];
 
 /* Initialize module */
 static int __init tresor_init(void)
 {
 	int retval;
 	retval = crypto_register_alg(&tresor_alg);
+	if (retval)
+		return retval;
+
+	retval = simd_register_skciphers_compat(tresor_skciphers,
+					     ARRAY_SIZE(tresor_skciphers),
+					     tresor_simd_skciphers);
+	if (retval)
+		goto unregister_cipher;
+
+	return 0;
+
+unregister_cipher:
+	crypto_unregister_alg(&tresor_alg);
 	return retval;
 }
 module_init(tresor_init);
@@ -156,6 +599,8 @@ module_init(tresor_init);
 /* Remove module */
 static void __exit tresor_fini(void)
 {
+	simd_unregister_skciphers(tresor_skciphers, ARRAY_SIZE(tresor_skciphers),
+				  tresor_simd_skciphers);
 	crypto_unregister_alg(&tresor_alg);
 }
 module_exit(tresor_fini);
diff -urp linux-5.4.50-ot.orig/crypto/testmgr.c linux-5.4.50-ot/crypto/testmgr.c
--- linux-5.4.50-ot.orig/crypto/testmgr.c	2020-07-03 22:01:03.633208256 -0700
+++ linux-5.4.50-ot/crypto/testmgr.c	2020-07-03 22:10:13.315999674 -0700
@@ -2500,8 +2500,11 @@ static int test_skcipher_vec_cfg(const c
 		crypto_skcipher_clear_flags(tfm,
 					    CRYPTO_TFM_REQ_FORBID_WEAK_KEYS);
 #ifdef CONFIG_CRYPTO_TRESOR
-	if (strstr(driver, "tresor"))
+	if (strstr(driver, "tresor")) {
 		tresor_setkey(vec->key);
+		if (strstr(driver, "xts"))
+			tresor_setkey_xts_tweak(vec->key + vec->klen/2);
+	}
 #endif
 	err = crypto_skcipher_setkey(tfm, vec->key, vec->klen);
 	if (err) {
@@ -4303,6 +4306,15 @@ static const struct alg_test_desc alg_te
 			.cipher = __VECS(sm4_ctr_tv_template)
 		}
 	}, {
+#ifdef CONFIG_CRYPTO_TRESOR
+		.alg = "ctr(tresor)",
+		.test = alg_test_skcipher,
+		.fips_allowed = 1,
+		.suite = {
+			.cipher = __VECS(aes_ctr_tv_template)
+		}
+	}, {
+#endif
 		.alg = "ctr(twofish)",
 		.test = alg_test_skcipher,
 		.suite = {
@@ -5149,6 +5161,16 @@ static const struct alg_test_desc alg_te
 			.cipher = __VECS(serpent_xts_tv_template)
 		}
 	}, {
+#ifdef CONFIG_CRYPTO_TRESOR
+		.alg = "xts(tresor)",
+		.generic_driver = "xts(ecb(aes-generic))",
+		.test = alg_test_skcipher,
+		.fips_allowed = 1,
+		.suite = {
+			.cipher = __VECS(aes_xts_tv_template)
+		}
+	}, {
+#endif
 		.alg = "xts(twofish)",
 		.generic_driver = "xts(ecb(twofish-generic))",
 		.test = alg_test_skcipher,
diff -urp linux-5.4.50-ot.orig/include/crypto/tresor.h linux-5.4.50-ot/include/crypto/tresor.h
--- linux-5.4.50-ot.orig/include/crypto/tresor.h	2020-07-03 22:01:03.552205315 -0700
+++ linux-5.4.50-ot/include/crypto/tresor.h	2020-07-03 22:03:13.376991846 -0700
@@ -10,10 +10,15 @@
 /* number of chars to clear memory */
 #define TRESOR_RANDOM_CHARS 4096
 
+struct tresor_ctx {
+	int key_length;
+};
+
 /* TRESOR core functionality (enc, dec, setkey) */
 void tresor_encrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src);
 void tresor_decrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src);
 void tresor_setkey(const u8 *in_key);
+void tresor_setkey_xts_tweak(const u8 *in_key);
 bool tresor_capable(void);
 
 #ifdef CONFIG_CRYPTO_TRESOR_PROMPT
--- linux-5.4.50-ot.orig/init/main.c.orig	2020-07-04 16:35:37.436394359 -0700
+++ linux-5.4.50-ot/init/main.c	2020-07-04 13:18:48.396844122 -0700
@@ -1225,6 +1225,8 @@ static noinline void __init kernel_init_
 	tresor_unlock_tests();
 	alg_test("ecb(tresor)", "ecb(tresor)", 0, 0);
 	alg_test("cbc(tresor)", "cbc(tresor)", 0, 0);
+	alg_test("ctr(tresor)", "ctr(tresor)", 0, 0);
+	alg_test("xts(tresor)", "xts(tresor)", 0, 0);
 	tresor_lock_tests();
 #endif
 
