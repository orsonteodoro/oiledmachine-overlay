Patch Status: Testing
Patch Author: Orson Teodoro <orsonteodoro@hotmail.com>
Date: Jul 23, 2020
Subject:  Add 256-bit AES support to TRESOR sse2 on 64-bit machines

This patch is for the Linux Kernel 5.7 series.

This patch only applies to the non-aesni version and depends on
tresor-glue-skcipher-cbc-ecb-ctr-xts-support-for-5.7-i686-v2.1.patch.

*This patch completes key expansion, encode, decode for 192 and 256-bit AES
for TRESOR sse2.
*This changes the key storage arrangement on 64 bit from column-major to
row-major order in debug registers.
*This also optimizes some XTS storage code.
*It may also fix compilation on i686 with more ifdef checks.
*Removed rc_tab.

v2 Fixes to ctr keysize.
   Deduped tresor_glue for duplicate encrypt/decrypt function pointer calls.
v1 Initial implementation.

todo: Check sanitation of key or temporary variables at the end of macro and
      function.

----
diff -urp linux-5.10.6-ot.orig/arch/x86/crypto/tresor_asm.S linux-5.10.6-ot/arch/x86/crypto/tresor_asm.S
--- linux-5.10.6-ot.orig/arch/x86/crypto/tresor_asm.S	2021-01-12 00:03:01.080207178 -0800
+++ linux-5.10.6-ot/arch/x86/crypto/tresor_asm.S	2021-01-12 00:56:13.399745562 -0800
@@ -1,6 +1,7 @@
 /***************************************************************************
  *
- * Cold boot resistant AES-128 for 32-bit/64-bit machines
+ * Cold boot resistant AES-128 for 32-bit machines with MMX & SSE2
+ *                     AES-128/192/256 for 64-bit machines with MMX & SSE2
  * 
  * Copyright (C) 2010	Tilo Mueller <tilo.mueller@informatik.uni-erlangen.de>
  * Copyright (C) 2012	Johannes Goetzfried <johannes@jgoetzfried.de>
@@ -24,14 +25,27 @@
 
 /* 128-bit SSE registers */
 .set	rstate,	%xmm0		/* AES state */
-.set	rhelp,	%xmm1		/* helping register */
+.set	rhelp,	%xmm1		/* helping register 1 */
 .set	rk1,	%xmm2		/* round key  1 */
 .set	rk2,	%xmm3		/* round key  2 */
 .set	rk3,	%xmm4		/* round key  3 */
 .set	rk4,	%xmm5		/* round key  4 */
 .set	rk5,	%xmm6		/* round key  5 */
 .set	rk10,	%xmm7		/* round key 10 */
-
+#ifdef __x86_64__
+/* Additional 128-bit SSE registers only on X86_64 */
+.set	rk6,	%xmm8		/* round key 6 */
+.set	rk7,	%xmm9		/* round key 7 */
+.set	rk8,	%xmm10		/* round key 8 */
+.set	rk9,	%xmm11		/* round key 9 */
+.set	rk11,	%xmm12		/* round key 11 */
+.set	rk12,	%xmm13		/* round key 12 */
+.set	rk13,	%xmm14		/* round key 13 */
+.set	rk14,	%xmm15		/* round key 14 */
+.set	rhelp2a, %mm0		/* helping register 2a */
+.set	rhelp2b, %mm1		/* helping register 2b */
+.set    rk0,    %xmm15
+#else
 /* 64-bit MMX registers */
 .set	rk6a,	%mm0		/* round key 6a */
 .set	rk6b,	%mm1		/* round key 6b */
@@ -41,14 +55,17 @@
 .set	rk8b,	%mm5		/* round key 8b */
 .set	rk9a,	%mm6		/* round key 9a */
 .set	rk9b,	%mm7		/* round key 9b */
+#endif
 
 /* 32-bit ^ 64-bit debug registers */
-/* low 32-bit values are for the crypto key */
-/* high 32-bit values are for the tweak key for xts */
-.set	db0,	%db0		/* hi(tweak key 0a) lo(round key 0a) */
-.set	db1,	%db1		/* hi(tweak key 0b) lo(round key 0b) */
-.set	db2,	%db2		/* hi(tweak key 1a) lo(round key 1a) */
-.set	db3,	%db3		/* hi(tweak key 1b) lo(round key 1b) */
+/* w is the 32-bit word of unexpanded key */
+/*                                 32-bit version    ; 64-bit version (cbc,ecb,ctr)  ;  64-bit 128-key and 256-xts   */
+/*                                cbc,ecb,ctr,no-xts ;  128/192/256 bit key support  ;                               */
+/*                                     128-key       ;         no xts support        ;                               */
+.set	db0,	%db0		/*        w0         ;             w1 w0             ;  w1            w0             */
+.set	db1,	%db1		/*        w1         ;             w3 w2             ;  w3            w2             */
+.set	db2,	%db2		/*        w2         ;             w5 w4             ;  tweak_key[1]  tweak_key[0]   */
+.set	db3,	%db3		/*        w3         ;             w7 w6             ;  tweak_key[3]  tweak_key[2]   */
 
 /* 32-bit GPR registers */
 .set	eax,	%eax
@@ -143,11 +160,6 @@ gfbd9e:	.long	0x00000000, 0x0b0d090e, 0x
 	.long	0x92b479a7, 0x99b970a9, 0x84ae6bbb, 0x8fa362b5
 	.long	0xbe805d9f, 0xb58d5491, 0xa89a4f83, 0xa397468d
 
-.align	128
-rc_tab:	.long	0x00000001, 0x00000002, 0x00000004, 0x00000008
-	.long	0x00000010, 0x00000020, 0x00000040, 0x00000080
-	.long	0x0000001b, 0x00000036
-
 /* external tables from generic aes code */
 .set	ft_tab, crypto_ft_tab
 .set	fl_tab, crypto_fl_tab
@@ -209,6 +221,16 @@ rc_tab:	.long	0x00000001, 0x00000002, 0x
 	pxor		%xmm5,%xmm5
 	pxor		%xmm6,%xmm6
 	pxor		%xmm7,%xmm7
+#ifdef __x86_64__
+	pxor		%xmm8,%xmm8
+	pxor		%xmm9,%xmm9
+	pxor		%xmm10,%xmm10
+	pxor		%xmm11,%xmm11
+	pxor		%xmm12,%xmm12
+	pxor		%xmm13,%xmm13
+	pxor		%xmm14,%xmm14
+	pxor		%xmm15,%xmm15
+#else
 	/* reset MMX registers */
 	pxor		%mm0,%mm0
 	pxor		%mm1,%mm1
@@ -218,6 +240,7 @@ rc_tab:	.long	0x00000001, 0x00000002, 0x
 	pxor		%mm5,%mm5
 	pxor		%mm6,%mm6
 	pxor		%mm7,%mm7
+#endif
 	/* leave */
 #ifdef __x86_64__
 #else
@@ -243,21 +266,29 @@ rc_tab:	.long	0x00000001, 0x00000002, 0x
 	pxor		rhelp,\sse
 .endm
 
-/* copy four 32-bit values from debug registers into one 128-bit sse register */
-.macro	dbg_to_sse dbg0 dbg1 dbg2 dbg3 sse
 #ifdef __x86_64__
-	movq		\dbg0,rax
-	movd		eax,\sse
-	movq		\dbg1,rax
-	movd		eax,rhelp
-	pslldq		$4,rhelp
-	pxor		rhelp,\sse
-	movq		\dbg2,rax
-	movd		eax,rhelp
-	pslldq		$8,rhelp
-	pxor		rhelp,\sse
-	movq		\dbg3,rax
+.macro	dbg_to_sse dbg0 dbg1 sse
+	/* w_(i+3) w_(i+2) w_(i+1) w_i = dbg1 dbg0 -> sse */
+	pxor	rhelp,rhelp
+	movq	\dbg0,rax
+	movq	rax,\sse
+	movq	\dbg1,rax
+	movq	rax,rhelp
+	pslldq	$8,rhelp
+	pxor	rhelp,\sse
+
+	pxor	rhelp,rhelp
+	xorq	rax,rax
+.endm
+
 #else
+/* copy four 32-bit values from debug registers into one 128-bit sse register */
+/* the 32-bit words are stored in the follwing order in \sse  w0 w1 w2 w3 ; w0 is high order word w3 is low order word */
+.macro	dbg_to_sse dbg0 dbg1 dbg2 dbg3 sse
+	/*
+	3  2  1  0  : index/address
+	w3 w2 w1 w0 = \sse
+	*/
 	movl		\dbg0,eax
 	movd		eax,\sse
 	movl		\dbg1,eax
@@ -269,41 +300,35 @@ rc_tab:	.long	0x00000001, 0x00000002, 0x
 	pslldq		$8,rhelp
 	pxor		rhelp,\sse
 	movl		\dbg3,eax
-#endif
 	movd		eax,rhelp
 	pslldq		$12,rhelp
 	pxor		rhelp,\sse
 .endm
+#endif
 
-/* copy four 32-bit values from debug registers into one 128-bit sse register */
-.macro	xts_dbg_to_sse dbg0 dbg1 dbg2 dbg3 sse
 #ifdef __x86_64__
-	movq		\dbg0,rax
-	rorq		$32,rax
-	movd		eax,\sse
-	movq		\dbg1,rax
-	rorq		$32,rax
-	movd		eax,rhelp
-	pslldq		$4,rhelp
+/* copy four 32-bit values from debug registers into one 128-bit sse register */
+.macro	xts_dbg_to_sse dbg2 dbg3 sse
+	/*
+	 db3   db2  : index/address
+	w3 w2 w1 w0 = \sse
+	*/
+	pxor		\sse,\sse
+
+	movq		\dbg3,rax
+	movq		rax,rhelp
 	pxor		rhelp,\sse
+
+	pslldq		$8,\sse
+
 	movq		\dbg2,rax
-	rorq		$32,rax
-	movd		eax,rhelp
-	pslldq		$8,rhelp
+	movq		rax,rhelp
 	pxor		rhelp,\sse
-	movq		\dbg3,rax
-	rorq		$32,rax
-#else
-#endif
-	movd		eax,rhelp
-	pslldq		$12,rhelp
-	pxor		rhelp,\sse
-#ifdef __x86_64__
+
+	pxor		rhelp,rhelp
 	xorq		rax,rax
-#else
-	xorl		eax,eax
-#endif
 .endm
+#endif
 
 /* copy four 32-bit general purpose registers into one 128-bit sse register */
 .macro	gpr_to_sse gpr0 gpr1 gpr2 gpr3 sse
@@ -340,46 +365,462 @@ rc_tab:	.long	0x00000001, 0x00000002, 0x
 #endif
 .endm
 
-/* generate next round key */
-.macro	generate_rk oldrk rk
-	movdqu		\oldrk,\rk
+/* generate next round key
+   in r0 r1 ; r0 for 128; r0 r1 for 192 and 256 AES
+   out r1 r2 ; r1 r2 for 192 and 256 AES ; r2 for AES 128
+   This macro is unrolled for one key expansion round.
+
+   AES-128
+                         exp round key   rcon
+   w3_rs    w2  w1  w0   r1/ri
+   w7_rs    w6  w5  w4   r1              0x01 *
+   w11_rs   w10 w9  w8   r2              0x02
+   w15_rs   w14 w13 w12  r3              0x04
+   w19_rs   w18 w17 w16  r4              0x08
+   w23_rs   w22 w21 w20  r5              0x10
+   w27_rs   w26 w25 w24  r6              0x20
+   w31_rs   w30 w29 w28  r7              0x40
+   w35_rs   w34 w33 w32  r8              0x80
+   w39_rs   w38 w37 w36  r9              0x1b
+   w43      w42 w41 w40  r10             0x36
+
+   *Important note about juggling in order to preserve convenience of
+   10 rounds == r10 and backwards compatibility with indexing on 32-bit machines.  Do
+   1.  decode first with expanded r1
+   2.  restore ri into r1
+   3.  decode with r1
+
+   Legend:
+   _rs means calculate RotWord(SubWord(wX))^rcon subexpression using that w_(i-1) for w_i
+
+   2 args required to obtain w[i-Nk] and update w[i]
+   r0 is read in
+   r1 is write out
+*/
+.macro	generate_rk_10 r0 r1 rcon
 	/*
-	k[0] ^= s_box(k[13]) ^ rc;
-	k[1] ^= s_box(k[14]);
-	k[2] ^= s_box(k[15]);
+	See page31 of FIPS-197 for indexing
+	each w is 32-bit word of expanded key
+	word ordering is increasing from right to left
+	byte ordering is increasing from right to left
+
+	 3  2  1  0 : address/index
+	w3 w2 w1 w0 = r0
+	*/
+	movdqu		\r0,\r1
+	/*
+	Starts expansion at w4
+
+	s_box rotation:
+
+	       vvvv
+	index: fedcba9876543210
+	                      |
+	after shifts:         |
+	                   fedcba9876543210
+	                      fedcba9876543210
+	                     fedcba9876543210
+	                    fedcba9876543210
+
+	fedc rotated into cfed
+
+	The k array below is w3 at the byte level
+
 	k[3] ^= s_box(k[12]);
+	k[2] ^= s_box(k[15]);
+	k[1] ^= s_box(k[14]);
+	k[0] ^= s_box(k[13]) ^ rc;
+
+	ror is done by shls
 	*/
-	ks_box		12,\rk,1
+	ks_box		12,\r1,1 /* start RotWord(SubWord(w3) */
 	shl		$8,edx
-	ks_box		15,\rk,0
+	ks_box		15,\r1,0
 	shl		$8,edx
-	ks_box		14,\rk,0
+	ks_box		14,\r1,0
 	shl		$8,edx
-	ks_box		13,\rk,0
+	ks_box		13,\r1,0 /* done RotWord(SubWord(w3) */
+	xorl		\rcon,edx /* RotWord(SubWord(w3) ^ rcon  */
+	movd		edx,rhelp
+
+	pxor		rhelp,\r1 /* w4 ; equivalent to rko[0] in aes.c's aes_expandkey (A) */
+	/*
+
+	Scaffolds.  The code should behave like this.
+
+	r1 = w3 w2 w1 w0
+	r1 = w3 w2 w1 w0^(RotWord(SubWord(w3))^rcon)
+
+                               N1
+	t = w3 w2 w1                             w0^(RotWord(SubWord(w3))^rcon)
+	t = w2 w1 w0^(RotWord(SubWord(w3))^rcon) 0                              a
+
+                               N2                                      N1
+	r1 = w3^w2 w2^w1                          w1^w0^(RotWord(SubWord(w3))^rcon) w0^(RotWord(SubWord(w3))^rcon)  B
+	t  = w1    w0^(RotWord(SubWord(w3))^rcon) 0                                 0                               b
+
+                                                             N3                                  N2                         N1
+	r1 = w3^w2^w1                       w2^w1^w0^(RotWord(SubWord(w3))^rcon) w1^w0^(RotWord(SubWord(w3))^rcon) w0^(RotWord(SubWord(w3))^rcon) C
+	t  = w0^(RotWord(SubWord(w3))^rcon) 0                                    0                                 0                              c
+
+code flow                      N4                   <-                 N3               <-                 N2             <-            N1
+	r1 = w3^w2^w1^w0^(RotWord(SubWord(w3))^rcon) w2^w1^w0^(RotWord(SubWord(w3))^rcon) w1^w0^(RotWord(SubWord(w3))^rcon) w0^(RotWord(SubWord(w3))^rcon) D
+expanded words in r1 =         w7                                      w6                                  w5                           w4
+
+	*/
+	movdqu		\r1,rhelp
+	pslldq		$4,rhelp /* a */
+	pxor		rhelp,\r1 /* w5 ; equivalent to rko[1] in aes.c's aes_expandkey ; B */
+	pslldq		$4,rhelp /* b */
+	pxor		rhelp,\r1 /* w6 ; equivalent to rko[2] in aes.c's aes_expandkey ; C */
+	pslldq		$4,rhelp /* c */
+	pxor		rhelp,\r1 /* w7 ; equivalent to rko[3] in aes.c's aes_expandkey ; D */
+.endm
+
 #ifdef __x86_64__
-	xorl		rc_tab(,rcx,4),edx
-#else
-	xorl		rc_tab(,ecx,4),edx
-#endif
+/*
+   AES-192
+
+                            exp round key            rcon     in/out
+   w3     w2   w1     w0    r0<->r14/ria                    + i  i
+   w7     w6   w5_rs  w4    r1/rib                   0x01   * iw i
+   w11_rs w10  w9     w8    r2                                   w
+   w15    w14  w13    w12   r3                       0x02        w i  i
+   w19    w18  w17_rs w16   r4                       0x04          iw i
+   w23_rs w22  w21    w20   r5                                        w
+   w27    w26  w25    w24   r6                       0x08             w  i  i
+   w31    w30  w29_rs w28   r7                       0x10                iw i
+   w35_rs w34  w33    w32   r8                                              w
+   w39    w38  w37    w36   r9                       0x20                   w i  i
+   w43    w42  w41_rs w40   r10                      0x40                     iw i
+   w47_rs w46  w45    w44   r11                                                  w
+   w51    w50  w49    w48   r12                      0x80                        w
+
+   +r0 is r13 temporarily
+
+   * Juggling note:
+   *Important note about juggling in order to preserve convenience of
+   12 rounds == r12 and backwards compatibility with indexing on 32-bit machines.  Do
+   1. decode first with expanded key r1
+   2. restore ria into r1
+   3. decode with r1
+
+   Legend:
+   _rs means calculate RotWord(SubWord(wX))^rcon subexpression using that w_(i-1) for w_i
+   i means read in
+   w means complete this line out completely for this round of key expansion
+
+   2 args required to obtain w[i-Nk] and update w[i]
+
+   r0 r1 are read in
+   r2 r3 are write out
+
+   rs_idx position to dump RotWord(SubWord()) in r2.
+   When fill_right is 1, it means expand colums 0 and 1 also
+   meaning expand w17, w16, w29, w28, w41, w40
+
+   index of 32-bit word in r2 regisiter:
+   3 2 1 0
+
+*/
+.macro	generate_rk_12 r0 r1 r2 r3 rcon rs_idx fill_right
+	/*
+	See page31 of FIPS-197 for indexing
+	each w is 32-bit word of expanded key
+	word ordering is increasing from right to left
+	byte ordering is increasing from right to left
+
+
+	*/
+	.if \rs_idx == 2
+
+	.if (\fill_right == 1)
+	movdqu		\r0,\rhelp
+	psrldq		$12,\rhelp
+
+	pxor		\rhelp,\r2 /* w16 */
+	movdqu		\r2,\rhelp
+
+	pslldq		$12,\rhelp
+	psrldq		$8,\rhelp
+
+	pxor		\rhelp,\r2 /* w17 */
+
+	.endif
+
+	/*
+
+	 3  2  1  0 : address/index
+	w3 w2 w1 w0 = r0
+	w7 w6 w5 w4 = r1
+
+	Starts expansion at w6
+
+	s_box rotation:
+
+	        3   2   1   0   : index/address of 4 32-bit words
+	           ^^^^vvvv
+	index: fedcba9876543210
+	                      |
+	after shifts:         |
+                   fedcba9x76543210
+                      fedcba9876543210
+                     fedcba9876543210
+                    fedcba9876543210
+
+	7654 rotated into 4765
+	ror is done by shls
+	*/
+
+	ks_box		4,\r1,1 /* start RotWord(SubWord(w5) */
+	shl		$8,edx
+	ks_box		7,\r1,0
+	shl		$8,edx
+	ks_box		6,\r1,0
+	shl		$8,edx
+	ks_box		5,\r1,0 /* done RotWord(SubWord(w5) */
+	xorl		\rcon,edx /* RotWord(SubWord(w5) ^ rcon  */
 	movd		edx,rhelp
-	pxor		rhelp,\rk
+
+	pslldq		$8,rhelp /* placed in ^^^^ in s_box rotation section or 2 index of words */
+	pxor		rhelp,\r2 /* w6 / w18 */
+
 	/*
-	for(cc = 4; cc < 16; cc += 4 )
-		k[cc + 0] ^= k[cc - 4];
-		k[cc + 1] ^= k[cc - 3];
-		k[cc + 2] ^= k[cc - 2];
-		k[cc + 3] ^= k[cc - 1];
+	Scaffolds.  The code should behave like this.
+
+
+	w3 w2                             w1 w0 = r0
+	w7 w6^(RotWord(SubWord(w5))^rcon) w5 w4 = r1
+
+	t= w7 w6^(RotWord(SubWord(w5))^rcon) w5 w4
+	t= w7                             w6^(RotWord(SubWord(w5))^rcon) 0 0 a1
+	t= w6^(RotWord(SubWord(w5))^rcon) 0                              0 0 a2
+
+code flow            <-             N2                   <-                   N1
+	r1 =         w7^w6^(RotWord(SubWord(w5))^rcon)           w6^(RotWord(SubWord(w5))^rcon)                          skip                                 skip
+expanded words in r2 =              w7                                        w6                                          w5                                   w4
+
 	*/
-	movdqu		\rk,rhelp
+
+	movdqu		\r2,rhelp
+
+	psrldq		$8,rhelp /* a1 */
+
+	pslldq		$12,rhelp /* a2 */
+	pxor		rhelp,\r2 /* w7 / w19 */
+
+	.endif
+	.if \rs_idx == 0
+
+	/*
+	Start expansion at w8
+	*/
+
+	movdqu		\r1,rhelp
+	psrldq		$12,rhelp
+
+	pxor		rhelp,\r2 /* w8 */
+
+	movdqu		\r2,rhelp
+
+	psrldq		$4,rhelp
+	pxor		rhelp,\r2 /* w9 */
+
+	psrldq		$4,rhelp
+	pxor		rhelp,\r2 /* w10 */
+
+	psrldq		$4,rhelp
+	pxor		rhelp,\r2 /* w11 */
+
+	/*
+	s_box rotation:
+
+	       vvvv        ^^^^
+	index: fedcba9876543210
+	                      |
+	after shifts:         |
+	                   fedcba9x76543210
+	                      fedcba9876543210
+	                     fedcba9876543210
+	                    fedcba9876543210
+
+	fedc rotated into cfed
+	ror is done by shls
+	*/
+
+	ks_box		12,\r2,1 /* start RotWord(SubWord(w11) */
+	shl		$8,edx
+	ks_box		15,\r2,0
+	shl		$8,edx
+	ks_box		14,\r2,0
+	shl		$8,edx
+	ks_box		13,\r2,0 /* done RotWord(SubWord(w11) */
+	xorl		\rcon,edx /* RotWord(SubWord(w11) ^ rcon  */
+	movd		edx,rhelp
+	pxor		rhelp,\r2 /* w12 */
+
+
+	/*
+	Scaffolds.  The code should behave like this.
+
+code flow            <-                     N4                           <-                         N3                            <-                      N2                           <-                   N1
+	r3 =                           w11^w10^w9^w8                                           w10^w9^w8^w7                                            w9^w8^w7                                            w8^w7
+expanded words in r3 =                     w11                                                     w10                                                    w9                                                w8
+
+code flow                                  N8                            <-                         N7                            <-                      N6                           <-                    N5
+	r4 =            w15^w14^w13^w12^(RotWord(SubWord(w11))^rcon)              w14^w13^w12^(RotWord(SubWord(w11))^rcon)                 w13^w12^(RotWord(SubWord(w11))^rcon)                w12^(RotWord(SubWord(w11))^rcon)
+expanded words in r4 = .                   w15                                                     w14                                                   w13                                                 w12
+
+	*/
+
+	movdqu		\r2,rhelp
+
+	psrldq		$4,rhelp
+	pxor            rhelp,\r2 /* w13 */
+
+	psrldq		$4,rhelp
+	pxor            rhelp,\r2 /* w14 */
+
+	psrldq		$4,rhelp
+	pxor            rhelp,\r2 /* w15 */
+
+	.endif
+.endm
+
+/*
+   AES-256
+                             exp round key     rcon
+   w3     w2  w1  w0         r1/ria
+   w7_rs  w6  w5  w4         r2/rib
+   w11_s  w10 w9  w8         r2                0x01 *
+   w15_rs w14 w13 w12        r3
+   w19_s  w18 w17 w16        r4                0x02
+   w23_rs w22 w21 w20        r5
+   w27_s  w26 w25 w24        r6                0x04
+   w31_rs w30 w29 w28        r7
+   w35_s  w34 w33 w32        r8                0x08
+   w39_rs w38 w37 w36        r9
+   w43_s  w42 w41 w40        r10               0x10
+   w47_rs w46 w45 w44        r11
+   w51_s  w50 w49 w48        r12               0x20
+   w55_rs w54 w53 w52        r13
+   w59    w58 w57 w56        r14               0x40
+
+   *Important note about juggling in order to preserve convenience of
+   14 rounds == r14 and backwards compatibility with indexing on 32-bit machines.  Do
+   1.  decode first with expanded key r2
+   2.  restore rib into r1
+   3.  decode with r1
+   4.  restore ria into r1
+   5.  decode with r1
+
+   Legend:
+   _rs means calculate RotWord(SubWord(wX))^rcon subexpression using that w_(i-1) for w_i
+   _s means calculate SubWord(wX) subexpression using that w_(i-1) for w_i
+
+   3 args required to obtain w[i-Nk] and update w[i]
+   rcon == 0x0 means calculate SubWord() only for r_i iff 256
+
+   r0 r1 are read in
+   r2 r3 are write out
+*/
+.macro	generate_rk_14 r0 r1 r2 r3 rcon
+	/*
+	See page31 of FIPS-197 for indexing
+	each w is 32-bit word of expanded key
+	word ordering is increasing from right to left
+	byte ordering is increasing from right to left
+
+	 3  2  1  0 : address/index
+	w3 w2 w1 w0 = r0
+	w7 w6 w5 w4 = r1
+	*/
+	movdqu		\r0,\r2
+	movdqu		\r1,\r3
+	/*
+	Starts expansion at w8
+
+	s_box rotation:
+
+	       vvvv
+	index: fedcba9876543210
+	                      |
+	after shifts:         |
+	                   fedcba9876543210
+	                      fedcba9876543210
+	                     fedcba9876543210
+	                    fedcba9876543210
+
+	fedc rotated into cfed
+
+	The k array below is w3/w7 at the byte level
+
+	k[3] ^= s_box(k[12]);
+	k[2] ^= s_box(k[15]);
+	k[1] ^= s_box(k[14]);
+	k[0] ^= s_box(k[13]) ^ rc;
+
+	ror is done by shls
+	*/
+	ks_box		12,\r2,1 /* start RotWord(SubWord(w7) */
+	shl		$8,edx
+	ks_box		15,\r2,0
+	shl		$8,edx
+	ks_box		14,\r2,0
+	shl		$8,edx
+	ks_box		13,\r2,0 /* done RotWord(SubWord(w7) */
+	xorl		\rcon,edx /* RotWord(SubWord(w7) ^ rcon  */
+	movd		edx,rhelp
+
+	pxor		rhelp,\r2 /* w8 ; equivalent to rko[0] in aes.c's aes_expandkey (A) */
+	/*
+	Scaffolds.  The code should behave like this.
+
+	r2 = w3 w2 w1 w0
+	r3 = w7 w6 w5 w4
+
+	r2 = w3 w2 w1 w0^(RotWord(SubWord(w7))^rcon)
+
+code flow                          N4                                     <-                         N3                           <-                            N2                        <-                      N1
+	r2 =           w3^w2^w1^w0^(RotWord(SubWord(w7))^rcon)                      w2^w1^w0^(RotWord(SubWord(w7))^rcon)                        w1^w0^(RotWord(SubWord(w7))^rcon)                      w0^(RotWord(SubWord(w7))^rcon) D
+expanded words in r2 =             w11                                                               w10                                                        w9                                                w8
+
+code flow                          N8                                     <-                         N7                           <-                            N6                        <-                      N5
+	r3 = w7^w6^w5^w4^SubWord(w3^w2^w1^w0^(RotWord(SubWord(w7))^rcon)) w6^w5^w4^SubWord(w3^w2^w1^w0^(RotWord(SubWord(w7))^rcon)) w5^w4^SubWord(w3^w2^w1^w0^(RotWord(SubWord(w7))^rcon)) w4^SubWord(w3^w2^w1^w0^(RotWord(SubWord(w7))^rcon))
+expanded words in r3 =             w15                                                               w14                                                       w13                                                w12
+	*/
+	movdqu		\r2,rhelp
+	pslldq		$4,rhelp /* a */
+	pxor		rhelp,\r2 /* w9 ; equivalent to rko[1] in aes.c's aes_expandkey ; B */
+	pslldq		$4,rhelp /* b */
+	pxor		rhelp,\r2 /* w10 ; equivalent to rko[2] in aes.c's aes_expandkey ; C */
+	pslldq		$4,rhelp /* c */
+	pxor		rhelp,\r2 /* w11 ; equivalent to rko[3] in aes.c's aes_expandkey ; D */
+
+	/* w11 w10 w9  w8
+	   w15 w14 w13 w12
+	ror is done by shls
+	*/
+	/* s_box(w_11) or s_box(w_(i-1)) -> edx for expanded keys w12, w20, w28, w36, w44, w52 in AES-256 */
+	ks_box		15,\r2,0
+	shl		$8,edx
+	ks_box		14,\r2,0
+	shl		$8,edx
+	ks_box		13,\r2,0
+	shl		$8,edx
+	ks_box		12,\r2,1
+	pxor		rhelp,rhelp
+	movd		edx,rhelp
+	pxor		rhelp,\r3 /* w12 (in 256) equivalent to rko[4] in aes.c's aes_expandkey */
+	movdqu		\r3,rhelp
 	pslldq		$4,rhelp
-	pxor		rhelp,\rk
+	pxor		rhelp,\r3 /* w13 (in 256) equivalent to rko[5] in aes.c's aes_expandkey */
 	pslldq		$4,rhelp
-	pxor		rhelp,\rk
+	pxor		rhelp,\r3 /* w14 (in 256) equivalent to rko[6] in aes.c's aes_expandkey */
 	pslldq		$4,rhelp
-	pxor		rhelp,\rk
-	/* increment RC (round constant) counter */
-	inc		ecx
+	pxor		rhelp,\r3 /* w15 (in 256) equivalent to rko[7] in aes.c's aes_expandkey */
 .endm
+#endif
 
 /* common code for inv_mix_column */
 .macro inv_mix_helper r l reg init
@@ -515,49 +956,124 @@ rc_tab:	.long	0x00000001, 0x00000002, 0x
 .endm
 
 /* generate AES-128 key schedule (rk1 to rk10) */
-.macro	key_schedule
+.macro key_schedule rounds
+	.if (\rounds == 10)
 	/* Generate rk1 to rk5 */
-	xorl		ecx,ecx
-	generate_rk	rk1,rk1
-	generate_rk	rk1,rk2
-	generate_rk	rk2,rk3
-	generate_rk	rk3,rk4
-	generate_rk	rk4,rk5
-	/* Generate rk6 to rk9 */
-	generate_rk	rk5,rk10
+	generate_rk_10	rk1,rk1,$0x1
+	generate_rk_10	rk1,rk2,$0x2
+	generate_rk_10	rk2,rk3,$0x4
+	generate_rk_10	rk3,rk4,$0x8
+	generate_rk_10	rk4,rk5,$0x10
+#ifdef __x86_64__
+	/* Generate rk5 to rk10 */
+	generate_rk_10	rk5,rk6,$0x20
+	generate_rk_10	rk6,rk7,$0x40
+	generate_rk_10	rk7,rk8,$0x80
+	generate_rk_10	rk8,rk9,$0x1b
+	generate_rk_10	rk9,rk10,$0x36
+#else
+	/* Generate rk5 to rk10 */
+	generate_rk_10	rk5,rk10,$0x20
 	sse_to_mmx	rk10,rk6a,rk6b
-	generate_rk	rk10,rk10
+	generate_rk_10	rk10,rk10,$0x40
 	sse_to_mmx	rk10,rk7a,rk7b	
-	generate_rk	rk10,rk10
+	generate_rk_10	rk10,rk10,$0x80
 	sse_to_mmx	rk10,rk8a,rk8b	
-	generate_rk	rk10,rk10
+	generate_rk_10	rk10,rk10,$0x1b
 	sse_to_mmx	rk10,rk9a,rk9b	
-	/* Generate rk10 */
-	generate_rk	rk10,rk10
+	generate_rk_10	rk10,rk10,$0x36
+#endif
+	.endif
+#ifdef __x86_64__
+	.if (\rounds == 12)
+	generate_rk_12	rk0,rk1,rk1,rk2,$0x1,$2,$0
+	generate_rk_12	rk0,rk1,rk2,rk3,$0x2,$0,$0
+	generate_rk_12	rk3,rk4,rk4,rk5,$0x4,$2,$1
+	generate_rk_12	rk3,rk4,rk5,rk6,$0x8,$0,$0
+	generate_rk_12	rk6,rk7,rk7,rk8,$0x10,$2,$1
+	generate_rk_12	rk6,rk7,rk8,rk9,$0x20,$0,$0
+	generate_rk_12	rk9,rk10,rk10,rk11,$0x40,$2,$1
+	generate_rk_12	rk9,rk10,rk11,rk12,$0x80,$0,$0
+	.endif
+	.if (\rounds == 14)
+	generate_rk_14	rk1,rk2,rk2,rk3,$0x1
+	generate_rk_14	rk2,rk3,rk4,rk5,$0x2
+	generate_rk_14	rk4,rk5,rk6,rk7,$0x4
+	generate_rk_14	rk6,rk7,rk8,rk9,$0x8
+	generate_rk_14	rk8,rk9,rk10,rk11,$0x10
+	generate_rk_14	rk10,rk11,rk12,rk13,$0x20
+	generate_rk_14	rk12,rk13,rk14,rhelp,$0x40
+	.endif
+#endif
 .endm
 
+#ifdef __x86_64__
+/* copy secret key from dbg regs into xmm regs */
+.macro read_key r0 r1
+	/*  3  2  1  0 :  1   0   : address/index */
+	/* w3 w2 w1 w0 = db1 db0 -> \r0 */
+	pxor	rhelp,rhelp
+	movq	db0,rax
+	movq	rax,\r0
+	movq	db1,rax
+	movq	rax,rhelp
+	pslldq	$8,rhelp
+	pxor	rhelp,\r0
+
+	/*  3  2  1  0 :  1   0   : address/index */
+	/* w7 w6 w5 w4 = db3 db2 -> \r1 */
+	pxor	rhelp,rhelp
+	movq	db2,rax
+	movq	rax,\r1
+	movq	db3,rax
+	movq	rax,rhelp
+	pslldq	$8,rhelp
+	pxor	rhelp,\r1
 
-/***************************************************************************
- *	  			CODE SEGMENT
- **************************************************************************/
-
-.text
-	.global		tresor_set_key
-	.global		tresor_encblk_128
-	.global		tresor_decblk_128
-	.global		tresor_set_key_xts_tweak_128
-	.global		tresor_encblk_128_xts_tweak
-	.extern		crypto_ft_tab
-	.extern		crypto_fl_tab
-	.extern		crypto_it_tab
-	.extern		crypto_il_tab
+	pxor	rhelp,rhelp
+	xorq	rax,rax
+.endm
+#endif
 
-/* void tresor_encblk_128(u8 *out, const u8 *in) */
-tresor_encblk_128:
+.macro tresor_encblk rounds
 	prolog 
+#ifdef __x86_64__
+	.if (\rounds == 10 || \rounds == 14)
+	read_key	rk1,rk2
+	pxor		rk1,rstate /* included as round */
+	.elseif (\rounds == 12)
+	read_key	rk0,rk1
+	pxor		rk0,rstate /* included as round */
+	.endif
+	key_schedule	\rounds
+	.if (\rounds == 14)
+	sse_to_mmx	rk1,rhelp2a,rhelp2b
+	dbg_to_sse	db2,db3,rk1
+	.endif
+	f_nround	rk1
+	.if (\rounds == 14)
+	mmx_to_sse	rhelp2a,rhelp2b,rk2
+	.endif
+	f_nround	rk2
+	f_nround	rk3
+	f_nround	rk4
+	f_nround	rk5
+	f_nround	rk6
+	f_nround	rk7
+	f_nround	rk8
+	f_nround	rk9
+	.if (\rounds >= 12)
+	f_nround	rk10
+	f_nround	rk11
+	.endif
+	.if (\rounds >= 14)
+	f_nround	rk12
+	f_nround	rk13
+	.endif
+#else
 	dbg_to_sse	db0,db1,db2,db3,rk1
-	pxor		rk1,rstate
-	key_schedule
+	pxor		rk1,rstate /* included as round */
+	key_schedule	\rounds
 	f_nround	rk1
 	f_nround	rk2
 	f_nround	rk3
@@ -567,15 +1083,46 @@ tresor_encblk_128:
 	f_nround_	rk7a,rk7b,rk5
 	f_nround_	rk8a,rk8b,rk5
 	f_nround_	rk9a,rk9b,rk5
-	f_lround	rk10
+#endif
+	f_lround	rk\rounds /* included as round */
 	epilog
+.endm
 
-/* void tresor_decblk_128(u8 *out, const u8 *in) */
-tresor_decblk_128:
+.macro tresor_decblk rounds
 	prolog
+#ifdef __x86_64__
+	.if (\rounds == 10 || \rounds == 14)
+	read_key	rk1,rk2
+	.elseif (\rounds == 12)
+	read_key	rk0,rk1
+	.endif
+	key_schedule	\rounds
+	pxor		rk\rounds,rstate /* included as round */
+	.if (\rounds >= 14)
+	i_nround	rk13
+	i_nround	rk12
+	.endif
+	.if (\rounds >= 12)
+	i_nround	rk11
+	i_nround	rk10
+	.endif
+	i_nround	rk9
+	i_nround	rk8
+	i_nround	rk7
+	i_nround	rk6
+	i_nround	rk5
+	i_nround	rk4
+	i_nround	rk3
+	i_nround	rk2
+	.if (\rounds == 14)
+	dbg_to_sse	db2,db3,rk1
+	.endif
+	i_nround	rk1
+	dbg_to_sse	db0,db1,rk1 /* Applies to AES 128/192/256 */
+#else
 	dbg_to_sse	db0,db1,db2,db3,rk1
-	key_schedule
-	pxor		rk10,rstate
+	key_schedule	\rounds
+	pxor		rk\rounds,rstate /* included as round */
 	i_nround_	rk9a,rk9b,rk10
 	i_nround_	rk8a,rk8b,rk10
 	i_nround_	rk7a,rk7b,rk10
@@ -586,40 +1133,154 @@ tresor_decblk_128:
 	i_nround	rk2
 	i_nround	rk1
 	dbg_to_sse	db0,db1,db2,db3,rk1
-	i_lround	rk1
+#endif
+	i_lround	rk1 /* included as round */
 	epilog
+.endm
+
+
+/***************************************************************************
+ *	  			CODE SEGMENT
+ **************************************************************************/
+
+.text
+	.global		tresor_set_key
+	.global		tresor_encblk_128
+	.global		tresor_decblk_128
+#ifdef __x86_64__
+	.global		tresor_encblk_192
+	.global		tresor_decblk_192
+	.global		tresor_encblk_256
+	.global		tresor_decblk_256
+#endif
+	.global		tresor_set_key_xts_tweak_128
+	.global		tresor_encblk_128_xts_tweak
+	.extern		crypto_ft_tab
+	.extern		crypto_fl_tab
+	.extern		crypto_it_tab
+	.extern		crypto_il_tab
+
+/* void tresor_encblk_128(u8 *out, const u8 *in) */
+tresor_encblk_128:
+	tresor_encblk	10
+/* void tresor_decblk_128(u8 *out, const u8 *in) */
+tresor_decblk_128:
+	tresor_decblk	10
+/* void tresor_encblk_192(u8 *out, const u8 *in) */
+tresor_encblk_192:
+#ifdef __x86_64__
+	tresor_encblk	12
+#else
+	ret
+#endif
+/* void tresor_decblk_128(u8 *out, const u8 *in) */
+tresor_decblk_192:
+#ifdef __x86_64__
+	tresor_decblk	12
+#else
+	ret
+#endif
+/* void tresor_encblk_256(u8 *out, const u8 *in) */
+tresor_encblk_256:
+#ifdef __x86_64__
+	tresor_encblk	14
+#else
+	ret
+#endif
+/* void tresor_decblk_256(u8 *out, const u8 *in) */
+tresor_decblk_256:
+#ifdef __x86_64__
+	tresor_decblk	14
+#else
+	ret
+#endif
 
 /* void tresor_encblk_128_xts_tweak(u8 *out, const u8 *in) */
 tresor_encblk_128_xts_tweak:
-	prolog 
-	xts_dbg_to_sse	db0,db1,db2,db3,rk1
+#ifdef __x86_64__
+	prolog
+	xts_dbg_to_sse	db2,db3,rk1
 	pxor		rk1,rstate
-	key_schedule
+	key_schedule 10
 	f_nround	rk1
 	f_nround	rk2
 	f_nround	rk3
 	f_nround	rk4
 	f_nround	rk5
-	f_nround_	rk6a,rk6b,rk5
-	f_nround_	rk7a,rk7b,rk5
-	f_nround_	rk8a,rk8b,rk5
-	f_nround_	rk9a,rk9b,rk5
+	f_nround	rk6
+	f_nround	rk7
+	f_nround	rk8
+	f_nround	rk9
 	f_lround	rk10
 	epilog
+#else
+	ret
+#endif
 
 /* void tresor_set_key(const u8 *in_key) */
 tresor_set_key:
 #ifdef __x86_64__
+	/*
+	stored as
+	1  0   : index/address
+	w1 w0 -> db0
+	w3 w2 -> db1
+	w5 w4 -> db2
+	w7 w6 -> db3
+
+	w3 w2 w1 w0 - db1 db0 -- are for 128-bit key
+	w5 w4 w3 w2 w1 w0 - db2 db1 db0 --  are for 192-bit key
+	w7 w6 w5 w4 w3 w2 w1 w0 - db3 db2 db1 db0 -- are for 256-bit key
+	*/
+	pushq		rbx
 	pushq		rax
 	movl		0(rdi),eax
-	movq		rax,db0
-	movl		4(rdi),eax
-	movq		rax,db1
+
+	/* w1 */
+	movl		4(rdi),ebx
+	shlq		$32,rbx
+	xorq		rax,rbx
+
+	movq		rbx,db0
+
+
+	/* w2 */
+	xorq		rax,rax
 	movl		8(rdi),eax
-	movq		rax,db2
+
+	/* w3 */
 	movl		12(rdi),eax
-	movq		rax,db3
+	shlq		$32,rax
+	xorq		rax,rbx
+
+	movq		rbx,db1
+
+
+	/* w4 */
+	xorq		rax,rax
+	movl		16(rdi),eax
+
+	/* w5 */
+	movl		20(rdi),eax
+	shlq		$32,rax
+	xorq		rax,rbx
+
+	movq		rbx,db2
+
+
+	/* w6 */
+	xorq		rax,rax
+	movl		24(rdi),eax
+
+	/* w7 */
+	movl		28(rdi),eax
+	shlq		$32,rax
+	xorq		rax,rbx
+
+	movq		rbx,db3
+
 	popq		rax
+	popq		rbx
 #else
 	movl		4(esp),edx
 	movl		0(edx),eax
@@ -637,42 +1298,34 @@ tresor_set_key:
 /* void tresor_set_key_xts_tweak_128(const u8 *in_key) */
 tresor_set_key_xts_tweak_128:
 #ifdef __x86_64__
+/*
+	tweak_key is 128-bit with 4 32-bit words
+                1               0       : address/index
+	db2 tweak_key[1]   tweak_key[0]
+	db3 tweak_key[3]   tweak_key[2]
+*/
 	pushq		rcx
 	pushq		rbx
 	pushq		rax
 
-	movl		0(rdi),eax
 	movq		$0x00000000ffffffff,rcx
-	andq		rcx,rax
-	rolq		$32,rax
-	movq		db0,rbx
-	andq		rcx,rbx
-	orq		rax,rbx
-	movq		rbx,db0
+	xorq		rbx,rbx
 
-	movl		4(rdi),eax
+	movl		0(rdi),eax
 	andq		rcx,rax
-	rolq		$32,rax
-	movq		db1,rbx
-	andq		rcx,rbx
-	orq		rax,rbx
-	movq		rbx,db1
+	movl		4(rdi),ebx
+	shlq		$32,rbx
+	xor		rbx,rax
+	movq		rax,db2
 
-	movl		8(rdi),eax
-	andq		rcx,rax
-	rolq		$32,rax
-	movq		db2,rbx
-	andq		rcx,rbx
-	orq		rax,rbx
-	movq		rbx,db2
+	xorq		rbx,rbx
 
-	movl		12(rdi),eax
+	movl		8(rdi),eax
 	andq		rcx,rax
-	rolq		$32,rax
-	movq		db3,rbx
-	andq		rcx,rbx
-	orq		rax,rbx
-	movq		rbx,db3
+	movl		12(rdi),ebx
+	shlq		$32,rbx
+	xorq		rbx,rax
+	movq		rax,db3
 
 	popq		rax
 	popq		rbx
Only in linux-5.10.6-ot/arch/x86/crypto: tresor_asm.S.orig
Only in linux-5.10.6-ot/arch/x86/crypto: tresor_asm.S.rej
diff -urp linux-5.10.6-ot.orig/arch/x86/crypto/tresor_glue.c linux-5.10.6-ot/arch/x86/crypto/tresor_glue.c
--- linux-5.10.6-ot.orig/arch/x86/crypto/tresor_glue.c	2021-01-12 00:03:01.080207178 -0800
+++ linux-5.10.6-ot/arch/x86/crypto/tresor_glue.c	2021-01-12 00:23:35.495297571 -0800
@@ -60,6 +60,12 @@ asmlinkage void tresor_set_key(const u8
 asmlinkage void tresor_set_key_xts_tweak_128(const u8 *in_key);
 asmlinkage void tresor_encblk_128(u8 *out, const u8 *in);
 asmlinkage void tresor_decblk_128(u8 *out, const u8 *in);
+#ifdef __x86_64__
+asmlinkage void tresor_encblk_192(u8 *out, const u8 *in);
+asmlinkage void tresor_decblk_192(u8 *out, const u8 *in);
+asmlinkage void tresor_encblk_256(u8 *out, const u8 *in);
+asmlinkage void tresor_decblk_256(u8 *out, const u8 *in);
+#endif
 asmlinkage void tresor_encblk_128_xts_tweak(u8 *out, const u8 *in);
 
 
@@ -75,8 +81,15 @@ static int tresor_setdummykey(struct cry
 {
 	struct crypto_aes_ctx *ctx = crypto_tfm_ctx(tfm);
 
+#ifdef __x86_64__
+	if( key_len != AES_KEYSIZE_128
+		&& key_len != AES_KEYSIZE_192
+		&& key_len != AES_KEYSIZE_256 )
+		return -EINVAL;
+#else
 	if( key_len != AES_KEYSIZE_128 )
 		return -EINVAL;
+#endif
 
 	ctx->key_length = key_len;
 	return 0;
@@ -104,17 +117,52 @@ static inline void tresor_epilog(unsigne
 	preempt_enable();
 }
 
+void __tresor_encrypt(int key_length, u8 *dst, const u8 *src)
+{
+	unsigned long irq_flags;
+	tresor_prolog(&irq_flags);
+	switch (key_length) {
+		case AES_KEYSIZE_128:
+			tresor_encblk_128(dst, src);
+			break;
+#ifdef __x86_64__
+		case AES_KEYSIZE_192:
+			tresor_encblk_192(dst, src);
+			break;
+		case AES_KEYSIZE_256:
+			tresor_encblk_256(dst, src);
+			break;
+#endif
+	}
+	tresor_epilog(&irq_flags);
+}
+
+void __tresor_decrypt(int key_length, u8 *dst, const u8 *src)
+{
+	tresor_prolog(&irq_flags);
+	switch (key_length) {
+		case AES_KEYSIZE_128:
+			tresor_decblk_128(dst, src);
+			break;
+#ifdef __x86_64__
+		case AES_KEYSIZE_192:
+			tresor_decblk_192(dst, src);
+			break;
+		case AES_KEYSIZE_256:
+			tresor_decblk_256(dst, src);
+			break;
+#endif
+	}
+	tresor_epilog(&irq_flags);
+}
 
 /*
  * Encrypt one block
  */
 void tresor_encrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src)
 {
-	unsigned long irq_flags;
-
-	tresor_prolog(&irq_flags);
-	tresor_encblk_128(dst, src);
-	tresor_epilog(&irq_flags);
+	struct crypto_aes_ctx *ctx = crypto_tfm_ctx(tfm);
+	__tresor_encrypt(ctx->key_length, dst, src);
 }
 
 
@@ -123,11 +171,9 @@ void tresor_encrypt(struct crypto_tfm *t
  */
 void tresor_decrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src)
 {
+	struct crypto_aes_ctx *ctx = crypto_tfm_ctx(tfm);
 	unsigned long irq_flags;
-
-	tresor_prolog(&irq_flags);
-	tresor_decblk_128(dst, src);
-	tresor_epilog(&irq_flags);
+	__tresor_decrypt(ctx->key_length, dst, src);
 }
 
 
@@ -169,26 +215,25 @@ static int tresor_skcipher_setkey(struct
 
 static int ecb_crypt(struct skcipher_request *req, bool enc)
 {
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);
 	struct skcipher_walk walk;
 	const unsigned int bsize = AES_BLOCK_SIZE;
 	unsigned int nbytes;
-	void (*f)(u8 *dst, const u8 *src);
 	int err;
-	unsigned long irq_flags;
 
 	err = skcipher_walk_virt(&walk, req, true);
 
-	f = (enc) ? tresor_encblk_128 : tresor_decblk_128;
-
 	while ((nbytes = walk.nbytes)) {
 		u8 *wsrc = walk.src.virt.addr;
 		u8 *wdst = walk.dst.virt.addr;
 
 		/* Handle leftovers */
 		do {
-			tresor_prolog(&irq_flags);
-			f(wdst, wsrc);
-			tresor_epilog(&irq_flags);
+			if (enc)
+				__tresor_encrypt(ctx->key_length, wdst, wsrc);
+			else
+				__tresor_decrypt(ctx->key_length, wdst, wsrc);
 
 			wsrc += bsize;
 			wdst += bsize;
@@ -214,11 +259,12 @@ static int ecb_decrypt(struct skcipher_r
 
 static int cbc_encrypt(struct skcipher_request *req)
 {
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);
 	const unsigned int bsize = AES_BLOCK_SIZE;
 	struct skcipher_walk walk;
 	unsigned int nbytes;
 	int err;
-	unsigned long irq_flags;
 
 	err = skcipher_walk_virt(&walk, req, false);
 
@@ -229,9 +275,7 @@ static int cbc_encrypt(struct skcipher_r
 
 		do {
 			u128_xor(dst, src, iv);
-			tresor_prolog(&irq_flags);
-			tresor_encblk_128((u8 *)dst, (u8 *)dst);
-			tresor_epilog(&irq_flags);
+			__tresor_encrypt(ctx->key_length, (u8 *)dst, (u8 *)dst);
 			iv = dst;
 			src++;
 			dst++;
@@ -253,7 +297,6 @@ static unsigned int __cbc_decrypt(struct
 	u128 *src = (u128 *)walk->src.virt.addr;
 	u128 *dst = (u128 *)walk->dst.virt.addr;
 	u128 last_iv;
-	unsigned long irq_flags;
 
 	/* Start of the last block. */
 	src += nbytes / bsize - 1;
@@ -263,9 +306,7 @@ static unsigned int __cbc_decrypt(struct
 
 	/* Handle leftovers */
 	for (;;) {
-		tresor_prolog(&irq_flags);
-		tresor_decblk_128((u8 *)dst, (u8 *)src);
-		tresor_epilog(&irq_flags);
+		__tresor_decrypt(ctx->key_length, (u8 *)dst, (u8 *)src);
 
 		nbytes -= bsize;
 		if (nbytes < bsize)
@@ -305,13 +346,12 @@ static void tresor_crypt_ctr(const void
 	u128 *dst = (u128 *)d;
 	const u128 *src = (const u128 *)s;
 	unsigned long irq_flags;
+	void (*f)(u8 *out, const u8 *in);
 
 	le128_to_be128(&ctrblk, iv);
 	le128_inc(iv);
 
-	tresor_prolog(&irq_flags);
-	tresor_encblk_128((u8 *)&ctrblk, (u8 *)&ctrblk);
-	tresor_epilog(&irq_flags);
+	__tresor_encrypt(ctx->key_length, (u8 *)&ctrblk, (u8 *)&ctrblk);
 
 	u128_xor(dst, src, (u128 *)&ctrblk);
 }
@@ -333,22 +373,14 @@ static int ctr_crypt(struct skcipher_req
 
 #ifdef CONFIG_X86_64
 /* 32-bit doesn't support 64-bit hardware breakpoint addresses */
-void __tresor_encrypt(const void *ctx, u8 *dst, const u8 *src) {
-	unsigned long irq_flags;
-
+void __xts_tresor_encrypt(const void *ctx, u8 *dst, const u8 *src) {
 	/* encrypt using the cipher key */
-	tresor_prolog(&irq_flags);
-	tresor_encblk_128(dst, src);
-	tresor_epilog(&irq_flags);
+	__tresor_encrypt(AES_KEYSIZE_128, dst, src);
 }
 
-void __tresor_decrypt(const void *ctx, u8 *dst, const u8 *src) {
-	unsigned long irq_flags;
-
+void __xts_tresor_decrypt(const void *ctx, u8 *dst, const u8 *src) {
 	/* decrypt using the cipher key */
-	tresor_prolog(&irq_flags);
-	tresor_decblk_128(dst, src);
-	tresor_epilog(&irq_flags);
+	__tresor_decrypt(AES_KEYSIZE_128, dst, src);
 }
 
 void __xts_tweak_tresor_encrypt(const void *ctx, u8 *dst, const u8 *src) {
@@ -391,13 +423,13 @@ static int xts_tresor_setkey(struct cryp
 static void tresor_xts_enc(const void *ctx, u8 *dst, const u8 *src, le128 *iv)
 {
 	glue_xts_crypt_128bit_one(ctx, dst, src, iv,
-				  __tresor_encrypt);
+				  __xts_tresor_encrypt);
 }
 
 static void tresor_xts_dec(const void *ctx, u8 *dst, const u8 *src, le128 *iv)
 {
 	glue_xts_crypt_128bit_one(ctx, dst, src, iv,
-				  __tresor_decrypt);
+				  __xts_tresor_decrypt);
 }
 
 static const struct common_glue_ctx tresor_enc_xts = {
@@ -457,7 +489,11 @@ static struct crypto_alg tresor_alg = {
 	.cra_u	= {
 		.cipher	= {
 			.cia_min_keysize	= AES_MIN_KEY_SIZE,
+#ifdef __x86_64__
+			.cia_max_keysize	= AES_MAX_KEY_SIZE,
+#else
 			.cia_max_keysize	= AES_MIN_KEY_SIZE,
+#endif
 			.cia_setkey		= tresor_setdummykey,
 			.cia_encrypt		= tresor_encrypt,
 			.cia_decrypt		= tresor_decrypt
@@ -477,7 +513,11 @@ static struct skcipher_alg tresor_skciph
 			.cra_module		= THIS_MODULE,
 		},
 		.min_keysize	= AES_MIN_KEY_SIZE,
+#ifdef __x86_64__
+		.max_keysize	= AES_MAX_KEY_SIZE,
+#else
 		.max_keysize	= AES_MIN_KEY_SIZE,
+#endif
 		.setkey		= tresor_skcipher_setkey,
 		.encrypt	= ecb_encrypt,
 		.decrypt	= ecb_decrypt,
@@ -492,7 +532,11 @@ static struct skcipher_alg tresor_skciph
 			.cra_module		= THIS_MODULE,
 		},
 		.min_keysize	= AES_MIN_KEY_SIZE,
+#ifdef __x86_64__
+		.max_keysize	= AES_MAX_KEY_SIZE,
+#else
 		.max_keysize	= AES_MIN_KEY_SIZE,
+#endif
 		.ivsize		= AES_BLOCK_SIZE,
 		.setkey		= tresor_skcipher_setkey,
 		.encrypt	= cbc_encrypt,
@@ -509,7 +553,7 @@ static struct skcipher_alg tresor_skciph
 			.cra_module		= THIS_MODULE,
 		},
 		.min_keysize	= AES_MIN_KEY_SIZE,
-		.max_keysize	= AES_MIN_KEY_SIZE,
+		.max_keysize	= AES_MAX_KEY_SIZE,
 		.ivsize		= AES_BLOCK_SIZE,
 		.chunksize	= AES_BLOCK_SIZE,
 		.setkey		= tresor_skcipher_setkey,
Only in linux-5.10.6-ot/arch/x86/crypto: tresor_glue.c.orig
Only in linux-5.10.6-ot/arch/x86/crypto: tresor_glue.c.rej
