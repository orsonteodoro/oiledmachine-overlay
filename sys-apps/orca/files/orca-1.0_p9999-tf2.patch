# Patch Status: Work In Progress (WIP)

rl-module.orig/agent.py:396:8: WARNING: *.save requires manual check. (This warning is only applicable if the code saves a tf.Keras model) Keras model.save now saves to the Tensorflow SavedModel format by default, instead of HDF5. To continue saving to HDF5, add the argument save_format='h5' to the save() function.

diff '--color=auto' -urpN orca-1.0_p9999.orig/rl-module/agent.py orca-1.0_p9999/rl-module/agent.py
--- orca-1.0_p9999.orig/rl-module/agent.py	2024-10-29 23:36:25.943113143 -0700
+++ orca-1.0_p9999/rl-module/agent.py	2024-10-29 23:38:26.286184672 -0700
@@ -49,21 +49,21 @@ class Actor():
 
 
     def train_var(self):
-        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)
+        return tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)
 
-    def build(self, s, is_training):
+    def build(self, s, learning):
 
-        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):
+        with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE):
 
-            h1 = tf.layers.dense(s, units=self.h1_shape, name='fc1')
-            h1 = tf.layers.batch_normalization(h1, training=is_training, scale=False)
+            h1 = tf.keras.layers.Dense(units=self.h1_shape, name='fc1')(s)
+            h1 = tf.keras.layers.BatchNormalization(scale=False)(h1, training=learning)
             h1 = tf.nn.leaky_relu(h1)
 
-            h2 = tf.layers.dense(h1, units=self.h2_shape,  name='fc2')
-            h2 = tf.layers.batch_normalization(h2, training=is_training, scale=False)
+            h2 = tf.keras.layers.Dense(units=self.h2_shape,  name='fc2')(h1)
+            h2 = tf.keras.layers.BatchNormalization(scale=False)(h2, training=learning)
             h2 = tf.nn.leaky_relu(h2)
 
-            output = tf.layers.dense(h2, units=self.a_dim, activation=tf.nn.tanh)
+            output = tf.keras.layers.Dense(units=self.a_dim, activation=tf.nn.tanh)(h2)
 
             scale_output = tf.multiply(output, self.action_scale)
 
@@ -84,23 +84,23 @@ class Critic():
 
 
     def train_var(self):
-        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)
+        return tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)
 
 
     def build(self, s, action):
 
-        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):
+        with tf.compat.v1.variable_scope(self.name, reuse=tf.compat.v1.AUTO_REUSE):
 
-            h1 = tf.layers.dense(s, units=self.h1_shape, activation=tf.nn.leaky_relu, name='fc1')
+            h1 = tf.keras.layers.Dense(units=self.h1_shape, activation=tf.nn.leaky_relu, name='fc1')(s)
 
-            h2 = tf.layers.dense(tf.concat([h1, action], -1), units=self.h2_shape, activation=tf.nn.leaky_relu, name='fc2')
-            output = tf.layers.dense(h2, units=1)
+            h2 = tf.keras.layers.Dense(units=self.h2_shape, activation=tf.nn.leaky_relu, name='fc2')(tf.concat([h1, action], -1))
+            output = tf.keras.layers.Dense(units=1)(h2)
 
         return output
 
 
 class Agent():
-    def __init__(self, s_dim, a_dim, h1_shape,h2_shape,gamma=0.995, batch_size=8, lr_a=1e-4, lr_c=1e-3, tau=1e-3, mem_size=1e5,action_scale=1.0, action_range=(-1.0, 1.0),
+    def __init__(self, learning, s_dim, a_dim, h1_shape,h2_shape,gamma=0.995, batch_size=8, lr_a=1e-4, lr_c=1e-3, tau=1e-3, mem_size=1e5,action_scale=1.0, action_range=(-1.0, 1.0),
                 noise_type=3, noise_exp=50000, summary=None,stddev=0.1, PER=False, alpha=0.6, CDQ=True, LOSS_TYPE='HUBERT'):
         self.PER = PER
         self.CDQ = CDQ
@@ -112,13 +112,14 @@ class Agent():
         self.gamma = gamma
         self.train_dir = './train_dir'
         self.step_epochs = tf.Variable(0, trainable=False, name='epoch')
-        self.global_step = tf.train.get_or_create_global_step(graph=None)
+        self.global_step = tf.compat.v1.train.get_or_create_global_step(graph=None)
+        self.learning = learning
 
 
-        self.s0 = tf.placeholder(tf.float32, shape=[None, self.s_dim], name='s0')
-        self.s1 = tf.placeholder(tf.float32, shape=[None, self.s_dim], name='s1')
-        self.is_training = tf.placeholder(tf.bool, name='Actor_is_training')
-        self.action = tf.placeholder(tf.float32, shape=[None, a_dim], name='action')
+        self.s0 = tf.compat.v1.placeholder(tf.float32, shape=[None, self.s_dim], name='s0')
+        self.s1 = tf.compat.v1.placeholder(tf.float32, shape=[None, self.s_dim], name='s1')
+        self.is_training = tf.compat.v1.placeholder(tf.bool, shape=(), name='Actor_is_training')
+        self.action = tf.compat.v1.placeholder(tf.float32, shape=[None, a_dim], name='action')
         self.noise_type = noise_type
         self.noise_exp = noise_exp
         self.action_range = action_range
@@ -151,7 +152,7 @@ class Agent():
         self.actor = Actor(self.s_dim, self.a_dim, action_scale=action_scale,h1_shape=self.h1_shape,h2_shape=self.h2_shape)
         self.critic = Critic(self.s_dim, self.a_dim, action_scale=action_scale,h1_shape=self.h1_shape,h2_shape=self.h2_shape)
         self.critic2 = Critic(self.s_dim, self.a_dim, action_scale=action_scale, name='critic2',h1_shape=self.h1_shape,h2_shape=self.h2_shape)
-        self.actor_out = self.actor.build(self.s0, self.is_training)
+        self.actor_out = self.actor.build(self.s0, self.learning)
         self.critic_out = self.critic.build(self.s0, self.action)
         self.critic_out2 = self.critic2.build(self.s0, self.action)
         self.critic_actor_out = self.critic.build(self.s0, self.actor_out)
@@ -161,7 +162,7 @@ class Agent():
         self.target_critic = Critic(self.s_dim, self.a_dim, action_scale=action_scale ,h1_shape=self.h1_shape,h2_shape=self.h2_shape,name='target_critic')
         self.target_critic2 = Critic(self.s_dim, self.a_dim, action_scale=action_scale, name='target_critic2',h1_shape=self.h1_shape,h2_shape=self.h2_shape)
 
-        self.target_actor_out = self.target_actor.build(self.s1, self.is_training)
+        self.target_actor_out = self.target_actor.build(self.s1, self.learning)
         self.target_actor_policy = self.get_target_actor_policy()
         self.target_critic_actor_out = self.target_critic.build(self.s1, self.target_actor_policy)
         self.target_critic_actor_out2 = self.target_critic2.build(self.s1, self.target_actor_policy)
@@ -174,14 +175,14 @@ class Agent():
         self.target_cri_init_op = self.target_init(self.target_critic.train_var(), self.critic.train_var())
         self.target_cri_init_op2 = self.target_init(self.target_critic2.train_var(), self.critic2.train_var())
 
-        self.extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
+        self.extra_update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)
 
-        self.terminal = tf.placeholder(tf.float32, shape=[None, 1], name='is_terminal')
-        self.reward = tf.placeholder(tf.float32, shape=[None, 1], name='reward')
+        self.terminal = tf.compat.v1.placeholder(tf.float32, shape=[None, 1], name='is_terminal')
+        self.reward = tf.compat.v1.placeholder(tf.float32, shape=[None, 1], name='reward')
         self.y = self.reward + self.gamma * (1-self.terminal) * self.target_critic_actor_out
         self.y2 = self.reward + self.gamma * (1-self.terminal) * self.target_critic_actor_out2
 
-        self.importance = tf.placeholder(tf.float32, [None,1], name='imporance_weights')
+        self.importance = tf.compat.v1.placeholder(tf.float32, [None,1], name='imporance_weights')
         self.td_error = self.critic_out - self.y
 
         self.summary_writer = summary
@@ -189,8 +190,8 @@ class Agent():
 
     def build_learn(self):
 
-        self.actor_optimizer = tf.train.AdamOptimizer(self.lr_a)
-        self.critic_optimizer = tf.train.AdamOptimizer(self.lr_c)
+        self.actor_optimizer = tf.compat.v1.train.AdamOptimizer(self.lr_a)
+        self.critic_optimizer = tf.compat.v1.train.AdamOptimizer(self.lr_c)
 
         self.actor_train_op = self.build_actor_train_op()
 
@@ -247,7 +248,7 @@ class Agent():
 
         tf.summary.scalar('Loss/actor_loss:', self.a_loss)
 
-        self.summary_op = tf.summary.merge_all()
+        self.summary_op = tf.compat.v1.summary.merge_all()
 
     def init_target(self):
         self.sess.run(self.target_act_init_op)
@@ -255,7 +256,7 @@ class Agent():
         self.sess.run(self.target_cri_init_op2)
 
     def get_target_actor_policy(self):
-        eps = tf.random_normal(tf.shape(self.target_actor_out), stddev=0.1)
+        eps = tf.random.normal(tf.shape(self.target_actor_out), stddev=0.1)
         eps = tf.clip_by_value(eps, -0.2, 0.2)
         t_a = self.target_actor_out + eps
         t_a = tf.clip_by_value(t_a, -1.0, 1.0)
@@ -284,13 +285,13 @@ class Agent():
         return self.actor_optimizer.minimize(self.a_loss, var_list=self.actor.train_var(), global_step = self.global_step)
 
     def target_init(self, target, vars):
-        return [tf.assign(target[i], vars[i]) for i in range(len(vars))]
+        return [tf.compat.v1.assign(target[i], vars[i]) for i in range(min(len(vars),len(target)))]
 
     def target_update_op(self, target, vars, tau):
-        return [tf.assign(target[i], vars[i] * tau + target[i] * (1 - tau)) for i in range(len(vars))]
+        return [tf.compat.v1.assign(target[i], vars[i] * tau + target[i] * (1 - tau)) for i in range(min(len(vars),len(target)))]
 
     def target_update_hard_op(self, target, vars):
-        return [tf.assign(target[i], vars[i]) for i in range(len(vars))]
+        return [tf.compat.v1.assign(target[i], vars[i]) for i in range(min(len(vars),len(target)))]
 
 
     def target_update(self):
@@ -343,7 +344,7 @@ class Agent():
 
     def train_step(self):
 
-        extra_update_ops = [v for v in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if
+        extra_update_ops = [v for v in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS) if
                             "actor" in v.name and "target" not in v.name]
 
         if self.PER == True:
@@ -388,7 +389,7 @@ class Agent():
 
 
     def log_tf(self, val, tag=None, step_counter=0):
-        summary = tf.Summary()
+        summary = tf.compat.v1.Summary()
         summary.value.add(tag= tag, simple_value=val)
         self.summary_writer.add_summary(summary, step_counter)
 
@@ -404,7 +405,7 @@ class Agent():
 
     def updat_step_epochs(self, epoch):
 
-        self.sess.run(tf.assign(self.step_epochs, epoch))
+        self.sess.run(tf.compat.v1.assign(self.step_epochs, epoch))
 
     def get_step_epochs(self):
 
diff '--color=auto' -urpN orca-1.0_p9999.orig/rl-module/d5.py orca-1.0_p9999/rl-module/d5.py
--- orca-1.0_p9999.orig/rl-module/d5.py	2024-10-29 23:36:25.947113112 -0700
+++ orca-1.0_p9999/rl-module/d5.py	2024-10-29 23:37:10.754768688 -0700
@@ -99,7 +99,7 @@ def evaluate_TCP(env, agent, epoch, summ
 
             if (step_counter+1) % params.dict['tb_interval'] == 0:
 
-                summary = tf.summary.Summary()
+                summary = tf.compat.v1.summary.Summary()
                 summary.value.add(tag='Eval/Step/0-Actions', simple_value=env.map_action(a))
                 summary.value.add(tag='Eval/Step/2-Reward', simple_value=r)
             summary_writer.add_summary(summary, eval_step_counter)
@@ -114,7 +114,7 @@ def evaluate_TCP(env, agent, epoch, summ
                 score_list.append(ep_r)
                 break
 
-    summary = tf.summary.Summary()
+    summary = tf.compat.v1.summary.Summary()
     summary.value.add(tag='Eval/Return', simple_value=np.mean(score_list))
     summary_writer.add_summary(summary, epoch)
 
@@ -171,7 +171,7 @@ def main():
         def is_actor_fn(i): return True
         global_variable_device = '/cpu'
         is_learner = False
-        server = tf.train.Server.create_local_server()
+        server = tf.distribute.Server.create_local_server()
         filters = []
     else:
 
@@ -198,7 +198,7 @@ def main():
                 })
 
 
-        server = tf.train.Server(cluster, job_name=config.job_name,
+        server = tf.distribute.Server(cluster, job_name=config.job_name,
                                 task_index=config.task)
         filters = [shared_job_device, local_job_device]
 
@@ -230,7 +230,7 @@ def main():
     with tf.Graph().as_default(),\
         tf.device(local_job_device + '/cpu'):
 
-        tf.set_random_seed(1234)
+        tf.compat.v1.set_random_seed(1234)
         random.seed(1234)
         np.random.seed(1234)
 
@@ -241,18 +241,19 @@ def main():
 
         if not os.path.exists(tfeventdir):
             os.makedirs(tfeventdir)
-        summary_writer = tf.summary.FileWriterCache.get(tfeventdir)
+        summary_writer = tf.compat.v1.summary.FileWriterCache.get(tfeventdir)
 
+        learning = not eval
         with tf.device(shared_job_device):
 
-            agent = Agent(s_dim, a_dim, batch_size=params.dict['batch_size'], summary=summary_writer,h1_shape=params.dict['h1_shape'],
+            agent = Agent(learning, s_dim, a_dim, batch_size=params.dict['batch_size'], summary=summary_writer,h1_shape=params.dict['h1_shape'],
                         h2_shape=params.dict['h2_shape'],stddev=params.dict['stddev'],mem_size=params.dict['memsize'],gamma=params.dict['gamma'],
                         lr_c=params.dict['lr_c'],lr_a=params.dict['lr_a'],tau=params.dict['tau'],PER=params.dict['PER'],CDQ=params.dict['CDQ'],
                         LOSS_TYPE=params.dict['LOSS_TYPE'],noise_type=params.dict['noise_type'],noise_exp=params.dict['noise_exp'])
 
             dtypes = [tf.float32, tf.float32, tf.float32, tf.float32, tf.float32]
             shapes = [[s_dim], [a_dim], [1], [s_dim], [1]]
-            queue = tf.FIFOQueue(10000, dtypes, shapes, shared_name="rp_buf")
+            queue = tf.queue.FIFOQueue(10000, dtypes, shapes, shared_name="rp_buf")
 
 
         if is_learner:
@@ -278,11 +279,11 @@ def main():
                     else:
                         env = GYM_Env_Wrapper(env_str, params)
 
-                    a_s0 = tf.placeholder(tf.float32, shape=[s_dim], name='a_s0')
-                    a_action = tf.placeholder(tf.float32, shape=[a_dim], name='a_action')
-                    a_reward = tf.placeholder(tf.float32, shape=[1], name='a_reward')
-                    a_s1 = tf.placeholder(tf.float32, shape=[s_dim], name='a_s1')
-                    a_terminal = tf.placeholder(tf.float32, shape=[1], name='a_terminal')
+                    a_s0 = tf.compat.v1.placeholder(tf.float32, shape=[s_dim], name='a_s0')
+                    a_action = tf.compat.v1.placeholder(tf.float32, shape=[a_dim], name='a_action')
+                    a_reward = tf.compat.v1.placeholder(tf.float32, shape=[1], name='a_reward')
+                    a_s1 = tf.compat.v1.placeholder(tf.float32, shape=[s_dim], name='a_s1')
+                    a_terminal = tf.compat.v1.placeholder(tf.float32, shape=[1], name='a_terminal')
                     a_buf = [a_s0, a_action, a_reward, a_s1, a_terminal]
 
 
@@ -305,13 +306,13 @@ def main():
         else:
             params.dict['ckptdir'] = tfeventdir
 
-        tfconfig = tf.ConfigProto(allow_soft_placement=True)
+        tfconfig = tf.compat.v1.ConfigProto(allow_soft_placement=True)
 
         if params.dict['single_actor_eval']:
-            mon_sess = tf.train.SingularMonitoredSession(
+            mon_sess = tf.compat.v1.train.SingularMonitoredSession(
                 checkpoint_dir=params.dict['ckptdir'])
         else:
-            mon_sess = tf.train.MonitoredTrainingSession(master=server.target,
+            mon_sess = tf.compat.v1.train.MonitoredTrainingSession(master=server.target,
                     save_checkpoint_secs=15,
                     save_summaries_secs=None,
                     save_summaries_steps=None,
