<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
  <maintainer type="person">
    <!-- Ebuild on the oiledmachine-overlay -->
    <email>orsonteodoro@hotmail.com</email>
    <name>Orson Teodoro</name>
  </maintainer>
  <longdescription>

    This use DRL (Deep Reinforce Learning) Agent and an existing congestion
    control to lower delays, to max throughput, to lower loss in existing and
    future TCP Congestion Controls.  Other benefits include lower CPU usage,
    better convergence towards the send link capacity and in unseen scenarios.

    DRL combines Reinforced Learning and Deep Neural Networks.

    This is a sender only TCP congestion control that changes the control window
    (cwnd) and the pacing rate.

    To run:

    # Standalone evaluation (for research only)
    ./orca-standalone-emulation.sh 44444

    # For a real network or use in production (ebuild mod)
    ORCA_SCHEME=cubic ./orca-real-network.sh

    # Actor-Learner for starting a new learner model for training
    ./orca.sh 1 44444

    # Actor-Learner for contining learner model training
    ./orca.sh 0 44444

    # Actor-Learner for sample test
    ./orca.sh 4 44444
  </longdescription>
  <upstream>
    <remote-id type="github">Soheil-ab/Orca</remote-id>
    <bugs-to>https://github.com/Soheil-ab/Orca/issues</bugs-to>
  </upstream>
  <use>
    <flag name="evaluate">
      Install dependencies required for evaluate.
    </flag>
    <flag name="build-models">
      Train DRL Agent for 6 hours each.
    </flag>
    <flag name="cellular-traces">
      Install celluar-traces to train the DRL or to playback in the evaluator.
    </flag>
    <flag name="kernel-patch">
      Install the kernel patch to apply on kernel sources.
    </flag>
  </use>
</pkgmetadata>
