<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
  <maintainer type="person">
    <!-- Ebuild on the oiledmachine-overlay -->
    <email>orsonteodoro@hotmail.com</email>
    <name>Orson Teodoro</name>
  </maintainer>
  <longdescription>

    This use DRL (Deep Reinforce Learning) Agent and an existing congestion
    control to lower delays, to max throughput, to lower loss in existing and
    future TCP Congestion Controls (CC).  Other benefits include lower CPU usage,
    better convergence towards the send link capacity and in unseen scenarios.

    DRL combines Reinforced Learning and Deep Neural Networks.

    This is a sender only TCP congestion control that changes the control window
    (cwnd) and the pacing rate.


    Comparison versus other TCP CCs and criticisms

      * Send performance is expected to be better than PCC.
      * Combined throughput and delay is optimal on the Production Possibilities
        Frontier (PPF) curve, balanced for throughput and latency which is good
        for general use.
      * Lower CPU utilization compared to PCC, or pure learning based CCs, but
        similar to the Congestion Control being used.
      * The model uses 4.4 MiB which is several orders of magnitude larger than
        the classical congestion control which typically is 100 - 1.3k lines of
        code.
      * PCC Vivace and Vegas are 4KiB compressed modules.
      * BBR, Cubic are an 8KiB compressed modules.
      * The TensorFlow dependency required for DRL agent is close to 700 MiB
        unpacked.
      * Training requires a lot of high end computer resources and out of reach
        for most individuals.
      * More build/packaging time investment versus kernel only CC for source
        based distros.
      * At this time of writing (2023), better algorithms do exist, but
        Orca still outperforms the default CC.
      * Better newer algorithms by others do not disclose usable source code.


    To run:

    # Standalone evaluation (for research only)
    ./orca-standalone-emulation.sh 44444

    # For a real network or use in production (ebuild mod)
    ORCA_SCHEME=cubic ./orca-real-network.sh

    # Actor-Learner for starting a new learner model for training
    ./orca.sh 1 44444

    # Actor-Learner for contining learner model training
    ./orca.sh 0 44444

    # Actor-Learner for sample test
    ./orca.sh 4 44444
  </longdescription>
  <upstream>
    <remote-id type="github">Soheil-ab/Orca</remote-id>
    <bugs-to>https://github.com/Soheil-ab/Orca/issues</bugs-to>
  </upstream>
  <use>
    <flag name="evaluate">
      Install dependencies required for evaluate.
    </flag>
    <flag name="build-models">
      Adds dependencies requried for training the DRL Agent for 6 hours.
      It requires a cluster, and server with the highest grade GPUs to complete.
      13.04% of the 400 cores combined should be learning servers and remaining
      actor clients.
    </flag>
    <flag name="cellular-traces">
      Install celluar-traces to train the DRL or to playback in the evaluator.
    </flag>
    <flag name="kernel-patch">
      Install the kernel patch to apply on kernel sources.
    </flag>
  </use>
</pkgmetadata>
