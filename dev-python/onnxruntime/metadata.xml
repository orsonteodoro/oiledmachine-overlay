<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
  <maintainer type="person">
    <!-- Ebuild on the oiledmachine-overlay -->
    <email>orsonteodoro@hotmail.com</email>
    <name>Orson Teodoro</name>
  </maintainer>
  <!-- Ebuild originally from cg overlay -->
  <upstream>
    <remote-id type="github">microsoft/onnxruntime</remote-id>
    <bugs-to>https://github.com/microsoft/onnxruntime/issues</bugs-to>
  </upstream>
  <use>
    <flag name="abseil-cpp">
      Use abseil-cpp to reduce memory allocations.
    </flag>
    <flag name="benchmark">
      Build microbenchmarks
    </flag>
    <flag name="cpu">
      Use the default execution provider.
    </flag>
    <flag name="cudnn">
      Add support for GPU accelerated deep neural network primitives for
      NVIDIA® GPUs.
    </flag>
    <flag name="extensions">
      Add support for custom operations.
    </flag>
    <flag name="composable-kernel">
      Enable use of the composable-kernel library.
    </flag>
    <flag name="javascript">
      Add JavaScript Node.js® bindings.
    </flag>
    <flag name="llvm">
      Build TVM with LLVM.
    </flag>
    <flag name="lto">
      Enable Link Time Optimization
    </flag>
    <flag name="migraphx">
      Build with GPU accelerated inference for AMD GPUs.
    </flag>
    <flag name="mimalloc">
      Enable use of a performance based memory allocator.
    </flag>
    <flag name="mpi">
      Add support for MPI for training.
    </flag>
    <flag name="neural-speed">
      Add support for optimized LLM inference on Intel® CPUs.
    </flag>
    <flag name="onednn">
      Use optimized JITable deep learning primitives for CPUs and/or
      Intel® GPUs.
    </flag>
    <flag name="openvino">
      Add support for optimized inference on CPUs, GPUs, Intel® NPUs.
    </flag>
    <flag name="openvino_targets_cpu">
      Add support for CPUs for OpenVINO™ support.
    </flag>
    <flag name="openvino_targets_cpu_np">
      Add support for CPUs for OpenVINO™ support with no graph partitioning.
    </flag>
    <flag name="openvino_targets_gpu">
      Add support for Intel® GPUs for OpenVINO™ support.
    </flag>
    <flag name="openvino_targets_gpu_np">
      Add support for Intel® GPUs for OpenVINO™ support with no graph partitioning.
    </flag>
    <flag name="openvino_targets_npu">
      Add support for Intel® NPUs for OpenVINO™ support.
    </flag>
    <flag name="openvino_targets_npu_np">
      Add support for Intel® NPUs for OpenVINO™ support with no graph partitioning.
    </flag>
    <flag name="quant">
      Add support for model compression for the quantization tool.
    </flag>
    <flag name="rocm">
      Add support for AMD GPUs.
    </flag>
    <flag name="tensorrt">
      Run inference on NVIDIA® GPUs.
    </flag>
    <flag name="tensorrt-oss-parser">
      Use the open sourced TensorRT static-library parser package for faster
      parser updates instead of the builtin shared library in the TensorRT
      package.
    </flag>
    <flag name="training">
      Enable full training
    </flag>
    <flag name="training-ort">
      Enable ORT [ONNX Runtime Training] APIs.
    </flag>
    <flag name="triton">
      Enable GPU kernels written in Triton language.
    </flag>
    <flag name="tvm">
      Build support for the TVM compiler for use in optimizing models.
    </flag>
    <flag name="system-eigen">
      Use the system's eigen package instead.
    </flag>
    <flag name="xnnpack">
      Build support for CPU accelerated inference primitives.
    </flag>
  </use>
</pkgmetadata>
